{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_kW45HnNVF9v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "683ff7e3-f051-4ef7-a1e5-53f15ed72453"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.11.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2024.10.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.8.30)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch-geometric) (4.12.2)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.6.1\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import torch_geometric\n",
        "except:\n",
        "    !pip install torch-geometric"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import networkx as nx\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch_geometric.data import HeteroData\n",
        "from torch_geometric.nn import GCNConv, SAGEConv\n",
        "from torch_geometric.loader import LinkNeighborLoader\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from tqdm.notebook import tqdm\n",
        "from torch_geometric.nn import HeteroConv, SAGEConv, Linear\n",
        "from torch_geometric.loader import NeighborLoader\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import copy\n",
        "\n",
        "import pickle\n",
        "import json\n",
        "\n",
        "# Using Pickle\n",
        "def save_data_pickle(papers_dict, topics_dict, filename=\"openalex_data.pkl\"):\n",
        "    \"\"\"Save the dictionaries using pickle\"\"\"\n",
        "    data = {\n",
        "        'papers': papers_dict,\n",
        "        'topics': topics_dict\n",
        "    }\n",
        "    with open(filename, 'wb') as f:\n",
        "        pickle.dump(data, f)\n",
        "\n",
        "def load_data_pickle(filename=\"openalex_data.pkl\"):\n",
        "    \"\"\"Load the dictionaries from pickle file\"\"\"\n",
        "    with open(filename, 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "    return data['papers'], data['topics']\n",
        "\n",
        "# Using JSON\n",
        "def save_data_json(papers_dict, topics_dict, filename=\"openalex_data.json\"):\n",
        "    \"\"\"Save the dictionaries using JSON\"\"\"\n",
        "    data = {\n",
        "        'papers': papers_dict,\n",
        "        'topics': topics_dict\n",
        "    }\n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump(data, f, indent=4)\n",
        "\n",
        "def load_data_json(filename=\"openalex_data.json\"):\n",
        "    \"\"\"Load the dictionaries from JSON file\"\"\"\n",
        "    with open(filename, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    return data['papers'], data['topics']"
      ],
      "metadata": {
        "id": "N7jmGvWRVfbk"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_abstract(inverted_index):\n",
        "    if inverted_index is not None:\n",
        "        abstract_words = []\n",
        "        max_index = max(max(positions) for positions in inverted_index.values())\n",
        "        abstract_words = [None] * (max_index + 1)\n",
        "        for word, positions in inverted_index.items():\n",
        "            for position in positions:\n",
        "                abstract_words[position] = word\n",
        "\n",
        "        abstract = \" \".join(\n",
        "            [word if word is not None else \"\" for word in abstract_words]\n",
        "        )\n",
        "        return abstract\n",
        "    else:\n",
        "        return \"Unavailable\"\n",
        "\n",
        "\n",
        "def fetch_openalex_data(paper_limit: int = 300, degree_limit: int = 1):\n",
        "    base_url = \"https://api.openalex.org/works\"\n",
        "    response = requests.get(base_url)\n",
        "\n",
        "    seed_works = response.json().get(\"results\")\n",
        "    ids = [\n",
        "        work[\"id\"].replace(\"openalex.org\", \"api.openalex.org/works\")\n",
        "        for work in seed_works\n",
        "    ]\n",
        "\n",
        "    # Remove after testing\n",
        "    # ids = [ids[0]]\n",
        "\n",
        "    papers_dict = {}\n",
        "    topics_dict = {}\n",
        "    degree = 0\n",
        "\n",
        "    while degree < degree_limit:\n",
        "        degree += 1\n",
        "        new_ids = []\n",
        "\n",
        "        for pid in ids:\n",
        "            if(len(papers_dict)) > 90000:\n",
        "                break\n",
        "            try:\n",
        "                response = requests.get(pid).json()\n",
        "                new_ids.append(response[\"id\"].replace(\"openalex.org\", \"api.openalex.org/works\"))\n",
        "                new_ids.extend([ref_work.replace(\"openalex.org\", \"api.openalex.org/works\") for ref_work in response[\"referenced_works\"]])\n",
        "                new_ids.extend([ref_work.replace(\"openalex.org\", \"api.openalex.org/works\") for ref_work in response[\"related_works\"]])\n",
        "                cited_by_url = response[\"cited_by_api_url\"]\n",
        "                cited_by_papers = requests.get(cited_by_url).json().get(\"results\")\n",
        "                new_ids.extend([work[\"id\"].replace(\"openalex.org\", \"api.openalex.org/works\") for work in cited_by_papers])\n",
        "\n",
        "                dic_topic = response.get('primary_topic')\n",
        "                dic_title = response.get(\"title\")\n",
        "                dic_abstract = response.get(\"abstract_inverted_index\")\n",
        "                topic = \"\"\n",
        "                title = \"\"\n",
        "                abstract = \"\"\n",
        "                if dic_topic is not None:\n",
        "                    topic = dic_topic.get(\"id\", \"\")\n",
        "                if dic_title is not None:\n",
        "                    title = dic_title\n",
        "                if dic_abstract is not None:\n",
        "                    abstract = get_abstract(dic_abstract)\n",
        "\n",
        "                papers_dict[pid] = {\n",
        "                    \"id\": pid,\n",
        "                    \"citation_count\": len(cited_by_papers),\n",
        "                    \"topic\": topic,\n",
        "                    \"title\": title,\n",
        "                    \"abstract\": abstract,\n",
        "                    \"cites\": [ref_work.replace(\"openalex.org\", \"api.openalex.org/works\") for ref_work in response[\"referenced_works\"]]\n",
        "                }\n",
        "                if degree == degree_limit:\n",
        "                    for work in [ref_work.replace(\"openalex.org\", \"api.openalex.org/works\") for ref_work in response[\"referenced_works\"]]:\n",
        "                        res = requests.get(work).json()\n",
        "                        if res is not None:\n",
        "                            dic_topic = res.get('primary_topic')\n",
        "                            dic_title = res.get(\"title\")\n",
        "                            dic_abstract = res.get(\"abstract_inverted_index\")\n",
        "                            topic = \"\"\n",
        "                            title = \"\"\n",
        "                            abstract = \"\"\n",
        "                            if dic_topic is not None:\n",
        "                                topic = dic_topic.get(\"id\", \"\")\n",
        "                            if dic_title is not None:\n",
        "                                title = dic_title\n",
        "                            if dic_abstract is not None:\n",
        "                                abstract = get_abstract(dic_abstract)\n",
        "                            papers_dict[work] = {\n",
        "                                \"id\": work,\n",
        "                                \"citation_count\": 0,\n",
        "                                \"topic\": topic,\n",
        "                                \"title\": title,\n",
        "                                \"abstract\": abstract,\n",
        "                                \"cites\": []\n",
        "                            }\n",
        "                            if dic_topic is not None:\n",
        "                                topics_dict[res[\"primary_topic\"][\"id\"]] = res[\"primary_topic\"][\"display_name\"]\n",
        "                            else:\n",
        "                                topics_dict[\"\"] = \"\"\n",
        "                if topic == \"\":\n",
        "                    topics_dict[\"\"] = \"\"\n",
        "                else:\n",
        "                    topics_dict[response[\"primary_topic\"][\"id\"]] = response[\"primary_topic\"][\"display_name\"]\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error fetching data for {pid}: {e}\")\n",
        "                continue\n",
        "\n",
        "        ids = new_ids\n",
        "    return papers_dict, topics_dict"
      ],
      "metadata": {
        "id": "3InKUOCfVhV0"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "papers_dict, topics_dict = load_data_pickle()"
      ],
      "metadata": {
        "id": "HpXv7FNGVi9g"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_edge_split(edge_index, num_nodes, val_ratio=0.1, test_ratio=0.1, message_ratio=0.5):\n",
        "    \"\"\"\n",
        "    Split edges into training message passing edges, training supervision edges,\n",
        "    validation edges, and test edges\n",
        "    \"\"\"\n",
        "    num_edges = edge_index.size(1)\n",
        "\n",
        "    # Create random permutation of edges\n",
        "    perm = torch.randperm(num_edges)\n",
        "    edge_index = edge_index[:, perm]\n",
        "\n",
        "    # Calculate split sizes\n",
        "    test_size = int(num_edges * test_ratio)\n",
        "    val_size = int(num_edges * val_ratio)\n",
        "    remaining_size = num_edges - test_size - val_size\n",
        "    message_size = int(remaining_size * message_ratio)\n",
        "    supervision_size = remaining_size - message_size\n",
        "\n",
        "    # Split edges\n",
        "    test_edges = edge_index[:, :test_size]\n",
        "    val_edges = edge_index[:, test_size:test_size + val_size]\n",
        "    train_msg_edges = edge_index[:, test_size + val_size:test_size + val_size + message_size]\n",
        "    train_sup_edges = edge_index[:, test_size + val_size + message_size:]\n",
        "\n",
        "    # Generate negative edges for each split\n",
        "    def sample_negative_edges(pos_edges, num_samples, existing_edges):\n",
        "        neg_edges = []\n",
        "        existing_edges_set = {(src.item(), dst.item()) for src, dst in existing_edges.t()}\n",
        "\n",
        "        while len(neg_edges) < num_samples:\n",
        "            src = torch.randint(0, num_nodes, (1,))\n",
        "            dst = torch.randint(0, num_nodes, (1,))\n",
        "            if src != dst and (src.item(), dst.item()) not in existing_edges_set:\n",
        "                neg_edges.append([src.item(), dst.item()])\n",
        "                existing_edges_set.add((src.item(), dst.item()))\n",
        "\n",
        "        return torch.tensor(neg_edges).t()\n",
        "\n",
        "    # Sample negative edges for each split\n",
        "    test_neg = sample_negative_edges(test_edges, test_size, edge_index)\n",
        "    val_neg = sample_negative_edges(val_edges, val_size, edge_index)\n",
        "    train_sup_neg = sample_negative_edges(train_sup_edges, supervision_size, edge_index)\n",
        "\n",
        "    return {\n",
        "        'train_msg': train_msg_edges,\n",
        "        'train_sup': (train_sup_edges, train_sup_neg),\n",
        "        'val': (val_edges, val_neg),\n",
        "        'test': (test_edges, test_neg)\n",
        "    }"
      ],
      "metadata": {
        "id": "8qF4uFox__OP"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import HeteroData\n",
        "from torch_geometric.nn import HeteroConv, GCNConv, SAGEConv, to_hetero\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "def create_edge_split(edge_index, num_nodes, val_ratio=0.1, test_ratio=0.1, message_ratio=0.5):\n",
        "    \"\"\"\n",
        "    Split edges into training message passing edges, training supervision edges,\n",
        "    validation edges, and test edges\n",
        "    \"\"\"\n",
        "    num_edges = edge_index.size(1)\n",
        "\n",
        "    # Create random permutation of edges\n",
        "    perm = torch.randperm(num_edges)\n",
        "    edge_index = edge_index[:, perm]\n",
        "\n",
        "    # Calculate split sizes\n",
        "    test_size = int(num_edges * test_ratio)\n",
        "    val_size = int(num_edges * val_ratio)\n",
        "    remaining_size = num_edges - test_size - val_size\n",
        "    message_size = int(remaining_size * message_ratio)\n",
        "    supervision_size = remaining_size - message_size\n",
        "\n",
        "    # Split edges\n",
        "    test_edges = edge_index[:, :test_size]\n",
        "    val_edges = edge_index[:, test_size:test_size + val_size]\n",
        "    train_msg_edges = edge_index[:, test_size + val_size:test_size + val_size + message_size]\n",
        "    train_sup_edges = edge_index[:, test_size + val_size + message_size:]\n",
        "\n",
        "    # Generate negative edges for each split\n",
        "    def sample_negative_edges(pos_edges, num_samples, existing_edges):\n",
        "        neg_edges = []\n",
        "        existing_edges_set = {(src.item(), dst.item()) for src, dst in existing_edges.t()}\n",
        "\n",
        "        while len(neg_edges) < num_samples:\n",
        "            src = torch.randint(0, num_nodes, (1,))\n",
        "            dst = torch.randint(0, num_nodes, (1,))\n",
        "            if src != dst and (src.item(), dst.item()) not in existing_edges_set:\n",
        "                neg_edges.append([src.item(), dst.item()])\n",
        "                existing_edges_set.add((src.item(), dst.item()))\n",
        "\n",
        "        return torch.tensor(neg_edges).t()\n",
        "\n",
        "    # Sample negative edges for each split\n",
        "    test_neg = sample_negative_edges(test_edges, test_size, edge_index)\n",
        "    val_neg = sample_negative_edges(val_edges, val_size, edge_index)\n",
        "    train_sup_neg = sample_negative_edges(train_sup_edges, supervision_size, edge_index)\n",
        "\n",
        "    return {\n",
        "        'train_msg': train_msg_edges,\n",
        "        'train_sup': (train_sup_edges, train_sup_neg),\n",
        "        'val': (val_edges, val_neg),\n",
        "        'test': (test_edges, test_neg)\n",
        "    }\n",
        "\n",
        "def create_hetero_graph(papers_dict, topics_dict):\n",
        "\n",
        "    # Initialize sentence transformer\n",
        "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "    # Create paper nodes\n",
        "    paper_ids = list(papers_dict.keys())\n",
        "    id_to_idx = {pid: idx for idx, pid in enumerate(paper_ids)}\n",
        "\n",
        "    # Create paper features using sentence transformer\n",
        "    paper_texts = []\n",
        "    for pid in paper_ids:\n",
        "        title = papers_dict[pid]['title']\n",
        "        abstract = papers_dict[pid]['abstract']\n",
        "        # Concatenate title and abstract\n",
        "        text = f\"{title} {abstract}\"\n",
        "        paper_texts.append(text)\n",
        "\n",
        "    # Get embeddings for all papers at once (more efficient)\n",
        "    paper_features = model.encode(paper_texts, convert_to_tensor=True)\n",
        "    data['paper'].x = paper_features\n",
        "    data['paper'].paper_ids = paper_ids\n",
        "\n",
        "    # Create topic nodes\n",
        "    topic_ids = list(topics_dict.keys())\n",
        "    topic_id_to_idx = {tid: idx for idx, tid in enumerate(topic_ids)}\n",
        "\n",
        "    # Create topic features using sentence transformer\n",
        "    topic_texts = [topics_dict[tid] for tid in topic_ids]\n",
        "    topic_features = model.encode(topic_texts, convert_to_tensor=True)\n",
        "    data['topic'].x = topic_features\n",
        "    data['topic'].topic_ids = topic_ids\n",
        "\n",
        "    # Create citation edges\n",
        "    edge_index_cites = []\n",
        "    for pid, paper_info in papers_dict.items():\n",
        "        src_idx = id_to_idx[pid]\n",
        "        for cited_id in paper_info['cites']:\n",
        "            if cited_id in id_to_idx:\n",
        "                dst_idx = id_to_idx[cited_id]\n",
        "                edge_index_cites.append([src_idx, dst_idx])\n",
        "\n",
        "    edge_index_cites = torch.tensor(edge_index_cites).t()\n",
        "\n",
        "    # Split citation edges\n",
        "    edge_splits = create_edge_split(edge_index_cites, len(paper_ids))\n",
        "\n",
        "    # Store splits in the data object\n",
        "    data['paper', 'cites', 'paper'].edge_index_splits = edge_splits\n",
        "    data['paper', 'cites', 'paper'].edge_index = edge_splits['train_msg']\n",
        "\n",
        "    # Create paper-topic edges\n",
        "    edge_index_topics = []\n",
        "    for pid, paper_info in papers_dict.items():\n",
        "        paper_idx = id_to_idx[pid]\n",
        "        topic_idx = topic_id_to_idx[paper_info['topic']]\n",
        "        edge_index_topics.append([paper_idx, topic_idx])\n",
        "\n",
        "    data['paper', 'has_topic', 'topic'].edge_index = torch.tensor(edge_index_topics).t()\n",
        "\n",
        "    return data\n",
        "\n",
        "class DirectionalPredictor(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        # Directional transform for source paper\n",
        "        self.source_transform = torch.nn.Linear(hidden_channels, hidden_channels)\n",
        "\n",
        "        # Directional transform for target paper\n",
        "        self.target_transform = torch.nn.Linear(hidden_channels, hidden_channels)\n",
        "\n",
        "        # Directional transform for source topic\n",
        "        self.source_topic_transform = torch.nn.Linear(hidden_channels, hidden_channels)\n",
        "\n",
        "        # Directional transform for target topic\n",
        "        self.target_topic_transform = torch.nn.Linear(hidden_channels, hidden_channels)\n",
        "\n",
        "        # Asymmetric scoring function\n",
        "        self.score = torch.nn.Sequential(\n",
        "            torch.nn.Linear(hidden_channels * 4, hidden_channels),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Dropout(0.5),\n",
        "            # Directional attention\n",
        "            torch.nn.Linear(hidden_channels, hidden_channels),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(hidden_channels, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, z_src, z_dst, z_topic_src, z_topic_dst):\n",
        "        # Transform source and target differently\n",
        "        h_src = self.source_transform(z_src)  # Source-specific transform\n",
        "        h_dst = self.target_transform(z_dst)  # Target-specific transform\n",
        "\n",
        "        # Transform topics differently based on source/target role\n",
        "        h_topic_src = self.source_topic_transform(z_topic_src)\n",
        "        h_topic_dst = self.target_topic_transform(z_topic_dst)\n",
        "\n",
        "        # Concatenate with explicit directional order\n",
        "        z = torch.cat([h_src, h_dst, h_topic_src, h_topic_dst], dim=-1)\n",
        "\n",
        "        return self.score(z).squeeze()\n",
        "\n",
        "class GNN(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = SAGEConv(-1, hidden_channels)\n",
        "        self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index).relu()\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "# class HeteroGNN(torch.nn.Module):\n",
        "#     def __init__(self, hidden_channels, data):\n",
        "#         super().__init__()\n",
        "#         self.gnn = GNN(hidden_channels)\n",
        "#         self.model = to_hetero(self.gnn, data.metadata())\n",
        "\n",
        "#         self.predictor = torch.nn.Sequential(\n",
        "#             torch.nn.Linear(hidden_channels * 4, hidden_channels),\n",
        "#             torch.nn.ReLU(),\n",
        "#             torch.nn.Dropout(0.5),\n",
        "#             torch.nn.Linear(hidden_channels, hidden_channels // 2),\n",
        "#             torch.nn.ReLU(),\n",
        "#             torch.nn.Linear(hidden_channels // 2, 1)\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x_dict, edge_index_dict):\n",
        "#         return self.model(x_dict, edge_index_dict)\n",
        "\n",
        "#     def predict_edge(self, z_dict, edge_index, paper_to_topic_edges):\n",
        "#         row, col = edge_index\n",
        "#         z_paper_src = z_dict['paper'][row]\n",
        "#         z_paper_dst = z_dict['paper'][col]\n",
        "\n",
        "#         topic_idx_src = self.get_topic_idx(row, paper_to_topic_edges)\n",
        "#         topic_idx_dst = self.get_topic_idx(col, paper_to_topic_edges)\n",
        "\n",
        "#         z_topic_src = z_dict['topic'][topic_idx_src]\n",
        "#         z_topic_dst = z_dict['topic'][topic_idx_dst]\n",
        "\n",
        "#         z = torch.cat([z_paper_src, z_paper_dst, z_topic_src, z_topic_dst], dim=-1)\n",
        "#         return self.predictor(z).squeeze()\n",
        "\n",
        "#     staticmethod\n",
        "#     def get_topic_idx(paper_idx, paper_to_topic_edges):\n",
        "#         topic_indices = []\n",
        "#         paper_idx_list = paper_idx.tolist()\n",
        "#         if not isinstance(paper_idx_list, list):\n",
        "#             paper_idx_list = [paper_idx_list]\n",
        "\n",
        "#         for idx in paper_idx_list:\n",
        "#             mask = (paper_to_topic_edges[0] == idx)\n",
        "#             topic_idx = paper_to_topic_edges[1][mask][0]\n",
        "#             topic_indices.append(topic_idx)\n",
        "\n",
        "#         return torch.tensor(topic_indices, device=paper_idx.device)\n",
        "class HeteroGNN(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels, data):\n",
        "        super().__init__()\n",
        "        self.gnn = GNN(hidden_channels)\n",
        "        self.model = to_hetero(self.gnn, data.metadata())\n",
        "\n",
        "        # Replace simple predictor with directional predictor\n",
        "        self.predictor = DirectionalPredictor(hidden_channels)\n",
        "\n",
        "    def forward(self, x_dict, edge_index_dict):\n",
        "        return self.model(x_dict, edge_index_dict)\n",
        "\n",
        "    def predict_edge(self, z_dict, edge_index, paper_to_topic_edges):\n",
        "        row, col = edge_index\n",
        "\n",
        "        # Get paper embeddings\n",
        "        z_paper_src = z_dict['paper'][row]\n",
        "        z_paper_dst = z_dict['paper'][col]\n",
        "\n",
        "        # Get topic embeddings\n",
        "        topic_idx_src = self.get_topic_idx(row, paper_to_topic_edges)\n",
        "        topic_idx_dst = self.get_topic_idx(col, paper_to_topic_edges)\n",
        "        z_topic_src = z_dict['topic'][topic_idx_src]\n",
        "        z_topic_dst = z_dict['topic'][topic_idx_dst]\n",
        "\n",
        "        # Use directional predictor\n",
        "        return self.predictor(z_paper_src, z_paper_dst, z_topic_src, z_topic_dst)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_topic_idx(paper_idx, paper_to_topic_edges):\n",
        "        topic_indices = []\n",
        "        paper_idx_list = paper_idx.tolist()\n",
        "        if not isinstance(paper_idx_list, list):\n",
        "            paper_idx_list = [paper_idx_list]\n",
        "\n",
        "        for idx in paper_idx_list:\n",
        "            mask = (paper_to_topic_edges[0] == idx)\n",
        "            topic_idx = paper_to_topic_edges[1][mask][0]\n",
        "            topic_indices.append(topic_idx)\n",
        "\n",
        "        return torch.tensor(topic_indices, device=paper_idx.device)\n",
        "\n",
        "def evaluate_edges(model, z_dict, edge_index_pos, edge_index_neg, paper_to_topic_edges):\n",
        "    \"\"\"Evaluate model on positive and negative edges\"\"\"\n",
        "    edge_index_all = torch.cat([edge_index_pos, edge_index_neg], dim=1)\n",
        "    labels = torch.cat([torch.ones(edge_index_pos.size(1)),\n",
        "                       torch.zeros(edge_index_neg.size(1))])\n",
        "\n",
        "    pred = model.predict_edge(z_dict, edge_index_all, paper_to_topic_edges)\n",
        "    pred_label = (pred > 0).float()\n",
        "\n",
        "    accuracy = accuracy_score(labels, pred_label)\n",
        "    precision = precision_score(labels, pred_label)\n",
        "    recall = recall_score(labels, pred_label)\n",
        "    f1 = f1_score(labels, pred_label)\n",
        "\n",
        "    return accuracy, precision, recall, f1\n",
        "\n",
        "def suggest_new_citations(model, data, k=5):\n",
        "    \"\"\"\n",
        "    Suggest k new citation pairs and check predictions in both directions.\n",
        "\n",
        "    Args:\n",
        "        model: Trained HeteroGNN model\n",
        "        data: HeteroData object containing paper information\n",
        "        k: Number of suggestions to return (default=5)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    paper_to_topic_edges = data['paper', 'has_topic', 'topic'].edge_index\n",
        "\n",
        "    # Get existing edges to avoid suggesting them\n",
        "    existing_edges = set()\n",
        "    edge_splits = data['paper', 'cites', 'paper'].edge_index_splits\n",
        "    for split_name, edges in edge_splits.items():\n",
        "        if isinstance(edges, tuple):  # Handle (pos_edge, neg_edge) tuples\n",
        "            edges = edges[0]  # Take only positive edges\n",
        "        for i in range(edges.size(1)):\n",
        "            existing_edges.add((edges[0, i].item(), edges[1, i].item()))\n",
        "\n",
        "    # Get number of papers\n",
        "    num_papers = data['paper'].x.size(0)\n",
        "\n",
        "    # Get node embeddings\n",
        "    with torch.no_grad():\n",
        "        node_embeddings = model(data.x_dict, data.edge_index_dict)\n",
        "\n",
        "    # Generate all possible paper pairs\n",
        "    suggestions = []\n",
        "    for i in range(num_papers):\n",
        "        for j in range(num_papers):\n",
        "            if i != j and (i, j) not in existing_edges:\n",
        "                # Check prediction A → B\n",
        "                edge_AB = torch.tensor([[i], [j]])\n",
        "                pred_AB = torch.sigmoid(model.predict_edge(\n",
        "                    node_embeddings, edge_AB, paper_to_topic_edges)).item()\n",
        "\n",
        "                # Check prediction B → A\n",
        "                edge_BA = torch.tensor([[j], [i]])\n",
        "                pred_BA = torch.sigmoid(model.predict_edge(\n",
        "                    node_embeddings, edge_BA, paper_to_topic_edges)).item()\n",
        "\n",
        "                # If either prediction is high enough, add to suggestions\n",
        "                if pred_AB > 0.8 or pred_BA > 0.8:  # Threshold of 0.8\n",
        "                    suggestions.append((i, j, pred_AB, pred_BA))\n",
        "\n",
        "    # Sort by maximum prediction score (either direction)\n",
        "    suggestions.sort(key=lambda x: max(x[2], x[3]), reverse=True)\n",
        "\n",
        "    # Print top k suggestions\n",
        "    print(f\"\\nTop {k} Citation Suggestions:\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    for src_idx, dst_idx, pred_AB, pred_BA in suggestions[:k]:\n",
        "        src_id = data['paper'].paper_ids[src_idx]\n",
        "        dst_id = data['paper'].paper_ids[dst_idx]\n",
        "\n",
        "        print(f\"Paper {src_id} → Paper {dst_id}:\")\n",
        "        print(f\"Forward prediction (A→B): {pred_AB:.4f}\")\n",
        "        print(f\"Reverse prediction (B→A): {pred_BA:.4f}\")\n",
        "\n",
        "        # Show which prediction is stronger\n",
        "        if pred_AB > pred_BA:\n",
        "            print(f\"Suggestion: Paper {src_id} should cite Paper {dst_id}\")\n",
        "        else:\n",
        "            print(f\"Suggestion: Paper {dst_id} should cite Paper {src_id}\")\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "def train_and_evaluate(data):\n",
        "    model = HeteroGNN(hidden_channels=64, data=data)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "    criterion = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "    edge_splits = data['paper', 'cites', 'paper'].edge_index_splits\n",
        "    paper_to_topic_edges = data['paper', 'has_topic', 'topic'].edge_index\n",
        "\n",
        "    best_val_f1 = 0\n",
        "    best_model = None\n",
        "\n",
        "    for epoch in range(100):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass using message passing edges\n",
        "        node_embeddings = model(data.x_dict, data.edge_index_dict)\n",
        "\n",
        "        # Get predictions for supervision edges\n",
        "        train_pos, train_neg = edge_splits['train_sup']\n",
        "        train_edge_index = torch.cat([train_pos, train_neg], dim=1)\n",
        "        train_labels = torch.cat([torch.ones(train_pos.size(1)),\n",
        "                                torch.zeros(train_neg.size(1))])\n",
        "\n",
        "        pred = model.predict_edge(node_embeddings, train_edge_index, paper_to_topic_edges)\n",
        "        loss = criterion(pred, train_labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_pos, val_neg = edge_splits['val']\n",
        "            val_acc, val_prec, val_rec, val_f1 = evaluate_edges(\n",
        "                model, node_embeddings, val_pos, val_neg, paper_to_topic_edges)\n",
        "\n",
        "            if val_f1 > best_val_f1:\n",
        "                best_val_f1 = val_f1\n",
        "                best_model = model.state_dict()\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f'Epoch {epoch+1:03d}, Loss: {loss:.4f}, Val F1: {val_f1:.4f}')\n",
        "\n",
        "    # Load best model and evaluate on test set\n",
        "    model.load_state_dict(best_model)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        final_embeddings = model(data.x_dict, data.edge_index_dict)\n",
        "        test_pos, test_neg = edge_splits['test']\n",
        "        test_acc, test_prec, test_rec, test_f1 = evaluate_edges(\n",
        "            model, final_embeddings, test_pos, test_neg, paper_to_topic_edges)\n",
        "\n",
        "        print(\"\\nTest Metrics:\")\n",
        "        print(f\"Accuracy: {test_acc:.4f}\")\n",
        "        print(f\"Precision: {test_prec:.4f}\")\n",
        "        print(f\"Recall: {test_rec:.4f}\")\n",
        "        print(f\"F1 Score: {test_f1:.4f}\")\n",
        "    suggest_new_citations(model, data)\n",
        "\n"
      ],
      "metadata": {
        "id": "yGr9J9SVABJv"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a6aH5ps9Eih_"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = create_hetero_graph(papers_dict, topics_dict)\n",
        "train_and_evaluate(data)"
      ],
      "metadata": {
        "id": "dGxmXvci5g1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vj8OT9Te55o4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}