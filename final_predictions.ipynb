{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "KhtTXj_aPUf2"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import torch_geometric\n",
    "except:\n",
    "    !pip install torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MoSbosujbhtC",
    "outputId": "aaa94c13-6a1e-4bbf-e3aa-92c46c9eab48"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a={'a':1}\n",
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Xu6yZHlNpbLr"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.nn import GCNConv, SAGEConv\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm.notebook import tqdm\n",
    "from torch_geometric.nn import HeteroConv, SAGEConv, Linear\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "k4IyAalFpcUE"
   },
   "outputs": [],
   "source": [
    "def get_abstract(inverted_index):\n",
    "    if inverted_index is not None:\n",
    "        abstract_words = []\n",
    "        max_index = max(max(positions) for positions in inverted_index.values())\n",
    "        abstract_words = [None] * (max_index + 1)\n",
    "        for word, positions in inverted_index.items():\n",
    "            for position in positions:\n",
    "                abstract_words[position] = word\n",
    "\n",
    "        abstract = \" \".join(\n",
    "            [word if word is not None else \"\" for word in abstract_words]\n",
    "        )\n",
    "        return abstract\n",
    "    else:\n",
    "        return \"Unavailable\"\n",
    "\n",
    "\n",
    "def fetch_openalex_data(paper_limit: int = 300, degree_limit: int = 1):\n",
    "    base_url = \"https://api.openalex.org/works\"\n",
    "    response = requests.get(base_url)\n",
    "\n",
    "    seed_works = response.json().get(\"results\")\n",
    "    ids = [\n",
    "        work[\"id\"].replace(\"openalex.org\", \"api.openalex.org/works\")\n",
    "        for work in seed_works\n",
    "    ]\n",
    "\n",
    "    # Remove after testing\n",
    "    # ids = [ids[0]]\n",
    "\n",
    "    papers_dict = {}\n",
    "    topics_dict = {}\n",
    "    degree = 0\n",
    "\n",
    "    while degree < degree_limit:\n",
    "        degree += 1\n",
    "        new_ids = []\n",
    "\n",
    "        for pid in ids:\n",
    "            if(len(papers_dict)) > 90000:\n",
    "                break\n",
    "            try:\n",
    "                response = requests.get(pid).json()\n",
    "                new_ids.append(response[\"id\"].replace(\"openalex.org\", \"api.openalex.org/works\"))\n",
    "                new_ids.extend([ref_work.replace(\"openalex.org\", \"api.openalex.org/works\") for ref_work in response[\"referenced_works\"]])\n",
    "                new_ids.extend([ref_work.replace(\"openalex.org\", \"api.openalex.org/works\") for ref_work in response[\"related_works\"]])\n",
    "                cited_by_url = response[\"cited_by_api_url\"]\n",
    "                cited_by_papers = requests.get(cited_by_url).json().get(\"results\")\n",
    "                new_ids.extend([work[\"id\"].replace(\"openalex.org\", \"api.openalex.org/works\") for work in cited_by_papers])\n",
    "\n",
    "                dic_topic = response.get('primary_topic')\n",
    "                dic_title = response.get(\"title\")\n",
    "                dic_abstract = response.get(\"abstract_inverted_index\")\n",
    "                topic = \"\"\n",
    "                title = \"\"\n",
    "                abstract = \"\"\n",
    "                if dic_topic is not None:\n",
    "                    topic = dic_topic.get(\"id\", \"\")\n",
    "                if dic_title is not None:\n",
    "                    title = dic_title\n",
    "                if dic_abstract is not None:\n",
    "                    abstract = get_abstract(dic_abstract)\n",
    "\n",
    "                papers_dict[pid] = {\n",
    "                    \"id\": pid,\n",
    "                    \"citation_count\": len(cited_by_papers),\n",
    "                    \"topic\": topic,\n",
    "                    \"title\": title,\n",
    "                    \"abstract\": abstract,\n",
    "                    \"cites\": [ref_work.replace(\"openalex.org\", \"api.openalex.org/works\") for ref_work in response[\"referenced_works\"]]\n",
    "                }\n",
    "                if degree == degree_limit:\n",
    "                    for work in [ref_work.replace(\"openalex.org\", \"api.openalex.org/works\") for ref_work in response[\"referenced_works\"]]:\n",
    "                        res = requests.get(work).json()\n",
    "                        if res is not None:\n",
    "                            dic_topic = res.get('primary_topic')\n",
    "                            dic_title = res.get(\"title\")\n",
    "                            dic_abstract = res.get(\"abstract_inverted_index\")\n",
    "                            topic = \"\"\n",
    "                            title = \"\"\n",
    "                            abstract = \"\"\n",
    "                            if dic_topic is not None:\n",
    "                                topic = dic_topic.get(\"id\", \"\")\n",
    "                            if dic_title is not None:\n",
    "                                title = dic_title\n",
    "                            if dic_abstract is not None:\n",
    "                                abstract = get_abstract(dic_abstract)\n",
    "                            papers_dict[work] = {\n",
    "                                \"id\": work,\n",
    "                                \"citation_count\": 0,\n",
    "                                \"topic\": topic,\n",
    "                                \"title\": title,\n",
    "                                \"abstract\": abstract,\n",
    "                                \"cites\": []\n",
    "                            }\n",
    "                            if dic_topic is not None:\n",
    "                                topics_dict[res[\"primary_topic\"][\"id\"]] = res[\"primary_topic\"][\"display_name\"]\n",
    "                            else:\n",
    "                                topics_dict[\"\"] = \"\"\n",
    "                if topic == \"\":\n",
    "                    topics_dict[\"\"] = \"\"\n",
    "                else:\n",
    "                    topics_dict[response[\"primary_topic\"][\"id\"]] = response[\"primary_topic\"][\"display_name\"]\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching data for {pid}: {e}\")\n",
    "                continue\n",
    "\n",
    "        ids = new_ids\n",
    "    return papers_dict, topics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "wdZ-nb2AqJXj"
   },
   "outputs": [],
   "source": [
    "# papers_dict, topics_dict = fetch_openalex_data(degree_limit=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "OI7M_pHvqODE"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.nn import GCNConv, SAGEConv\n",
    "import torch.nn.functional as F\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "def create_heterogeneous_graph(papers_dict, topics_dict):\n",
    "    # Collect unique topics\n",
    "    unique_topics = set(paper_info['topic'] for paper_info in papers_dict.values() if paper_info['topic'])\n",
    "\n",
    "    # Create mappings\n",
    "    paper_ids = list(papers_dict.keys())\n",
    "    topic_ids = list(unique_topics)\n",
    "\n",
    "    # Create index mappings\n",
    "    paper_id_to_idx = {pid: idx for idx, pid in enumerate(paper_ids)}\n",
    "    topic_id_to_idx = {tid: idx + len(paper_ids) for idx, tid in enumerate(topic_ids)}\n",
    "\n",
    "    # Use SentenceTransformer for text embeddings\n",
    "    text_embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "    # Prepare node features\n",
    "    total_nodes = len(paper_ids) + len(topic_ids)\n",
    "    feature_dim = 384  # SentenceTransformer embedding dimension\n",
    "    x = torch.zeros(total_nodes, feature_dim)\n",
    "\n",
    "    # Populate paper features\n",
    "    for paper_id in paper_ids:\n",
    "        paper_info = papers_dict[paper_id]\n",
    "        text = f\"{paper_info['title']} {paper_info['abstract']}\"\n",
    "        embedding = text_embedder.encode(text, convert_to_tensor=True)\n",
    "        x[paper_id_to_idx[paper_id]] = embedding\n",
    "\n",
    "    # Populate topic features\n",
    "    for topic_id in topic_ids:\n",
    "        topic_name = topics_dict.get(topic_id, topic_id)\n",
    "        embedding = text_embedder.encode(topic_name, convert_to_tensor=True)\n",
    "        x[topic_id_to_idx[topic_id]] = embedding\n",
    "\n",
    "    # Prepare edge indices\n",
    "    citation_src, citation_dst = [], []\n",
    "    topic_paper_src, topic_paper_dst = [], []\n",
    "\n",
    "    # Create citation edges\n",
    "    for paper_id in paper_ids:\n",
    "        paper_info = papers_dict[paper_id]\n",
    "        src_idx = paper_id_to_idx[paper_id]\n",
    "        for cited_paper in paper_info['cites']:\n",
    "            if cited_paper in paper_id_to_idx:\n",
    "                dst_idx = paper_id_to_idx[cited_paper]\n",
    "                citation_src.append(src_idx)\n",
    "                citation_dst.append(dst_idx)\n",
    "\n",
    "    # Create topic-paper edges\n",
    "    for paper_id in paper_ids:\n",
    "        paper_info = papers_dict[paper_id]\n",
    "        if paper_info['topic'] in topic_id_to_idx:\n",
    "            paper_idx = paper_id_to_idx[paper_id]\n",
    "            topic_idx = topic_id_to_idx[paper_info['topic']]\n",
    "\n",
    "            # Bidirectional edges\n",
    "            topic_paper_src.append(paper_idx)\n",
    "            topic_paper_dst.append(topic_idx)\n",
    "            topic_paper_src.append(topic_idx)\n",
    "            topic_paper_dst.append(paper_idx)\n",
    "\n",
    "    # Create edge indices\n",
    "    citation_edge_index = torch.tensor([citation_src, citation_dst], dtype=torch.long)\n",
    "    topic_paper_edge_index = torch.tensor([topic_paper_src, topic_paper_dst], dtype=torch.long)\n",
    "\n",
    "    # Create HeteroData\n",
    "    data = HeteroData()\n",
    "    data['paper', 'cites', 'paper'].edge_index = citation_edge_index\n",
    "    data['paper', 'has_topic', 'topic'].edge_index = topic_paper_edge_index\n",
    "    data['topic', 'has_paper', 'paper'].edge_index = topic_paper_edge_index.flip(0)\n",
    "    data.x = x\n",
    "\n",
    "    return data, paper_id_to_idx, topic_id_to_idx\n",
    "\n",
    "def print_graph_statistics(graph_data, papers_dict, topics_dict, edge_splits):\n",
    "    \"\"\"Print detailed statistics about the graph structure.\"\"\"\n",
    "\n",
    "    print(\"\\n=== Graph Statistics ===\")\n",
    "\n",
    "    # Node statistics\n",
    "    num_papers = len(papers_dict)\n",
    "    num_topics = len(topics_dict)\n",
    "    print(\"\\nNode Counts:\")\n",
    "    print(f\"Papers: {num_papers}\")\n",
    "    print(f\"Topics: {num_topics}\")\n",
    "    print(f\"Total Nodes: {num_papers + num_topics}\")\n",
    "\n",
    "    # Edge statistics\n",
    "    paper_edges = graph_data['paper', 'cites', 'paper'].edge_index\n",
    "    topic_edges = graph_data['paper', 'has_topic', 'topic'].edge_index\n",
    "\n",
    "    print(\"\\nTotal Edge Counts:\")\n",
    "    print(f\"Citations (paper → paper): {paper_edges.size(1)}\")\n",
    "    print(f\"Topic Associations (paper ↔ topic): {topic_edges.size(1)}\")\n",
    "    print(f\"Total Edges: {paper_edges.size(1) + topic_edges.size(1)}\")\n",
    "\n",
    "    # Edge split statistics\n",
    "    (paper_message, paper_supervision, paper_val, paper_test,\n",
    "     topic_message, topic_supervision, topic_val, topic_test) = edge_splits\n",
    "\n",
    "    print(\"\\nCitation Edge Splits:\")\n",
    "    print(f\"Message Edges: {paper_message.size(1)}\")\n",
    "    print(f\"Supervision Edges: {paper_supervision.size(1)}\")\n",
    "    print(f\"Validation Edges: {paper_val.size(1)}\")\n",
    "    print(f\"Test Edges: {paper_test.size(1)}\")\n",
    "    print(f\"Total: {paper_message.size(1) + paper_supervision.size(1) + paper_val.size(1) + paper_test.size(1)}\")\n",
    "\n",
    "    print(\"\\nTopic Association Edge Splits:\")\n",
    "    print(f\"Message Edges: {topic_message.size(1)}\")\n",
    "    print(f\"Supervision Edges: {topic_supervision.size(1)}\")\n",
    "    print(f\"Validation Edges: {topic_val.size(1)}\")\n",
    "    print(f\"Test Edges: {topic_test.size(1)}\")\n",
    "    print(f\"Total: {topic_message.size(1) + topic_supervision.size(1) + topic_val.size(1) + topic_test.size(1)}\")\n",
    "\n",
    "    # Additional statistics\n",
    "    print(\"\\nGraph Density:\")\n",
    "    possible_citations = num_papers * (num_papers - 1)  # Directed edges\n",
    "    citation_density = paper_edges.size(1) / possible_citations\n",
    "    print(f\"Citation Density: {citation_density:.6f}\")\n",
    "\n",
    "    possible_topic_associations = num_papers * num_topics\n",
    "    topic_density = topic_edges.size(1) / possible_topic_associations\n",
    "    print(f\"Topic Association Density: {topic_density:.6f}\")\n",
    "\n",
    "    # Topic distribution\n",
    "    topic_counts = {}\n",
    "    for paper_info in papers_dict.values():\n",
    "        topic = paper_info['topic']\n",
    "        topic_counts[topic] = topic_counts.get(topic, 0) + 1\n",
    "\n",
    "    print(\"\\nTopic Distribution:\")\n",
    "    print(\"Top 5 topics by paper count:\")\n",
    "    sorted_topics = sorted(topic_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    for topic_id, count in sorted_topics[:5]:\n",
    "        topic_name = topics_dict.get(topic_id, 'Unknown')\n",
    "        print(f\"Topic: {topic_name}, Papers: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "1j3ULtslUbmR"
   },
   "outputs": [],
   "source": [
    "class HeteroGCNMultiPredictor(torch.nn.Module):\n",
    "    def __init__(self, feature_dim, hidden_channels=64):\n",
    "        super().__init__()\n",
    "\n",
    "        # Graph convolution layers\n",
    "        self.conv1 = GCNConv(feature_dim, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "\n",
    "        # Link predictor for paper citations\n",
    "        self.citation_predictor = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_channels * 2, hidden_channels),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_channels, 1)\n",
    "        )\n",
    "\n",
    "        # Link predictor for topic associations\n",
    "        self.topic_predictor = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_channels * 2, hidden_channels),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_channels, 1)\n",
    "        )\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "    def decode_citations(self, z, edge_label_index):\n",
    "        row, col = edge_label_index\n",
    "        z_src = z[row]\n",
    "        z_dst = z[col]\n",
    "        z_combined = torch.cat([z_src, z_dst], dim=-1)\n",
    "        return self.citation_predictor(z_combined).squeeze()\n",
    "\n",
    "    def decode_topics(self, z, edge_label_index):\n",
    "        row, col = edge_label_index\n",
    "        z_src = z[row]\n",
    "        z_dst = z[col]\n",
    "        z_combined = torch.cat([z_src, z_dst], dim=-1)\n",
    "        return self.topic_predictor(z_combined).squeeze()\n",
    "\n",
    "def split_edges(edge_index, val_ratio=0.1, test_ratio=0.1, message_ratio=0.5):\n",
    "    \"\"\"\n",
    "    Split edges into training message edges, training supervision edges, validation edges, and test edges.\n",
    "    message_ratio determines split between message and supervision edges within training set.\n",
    "    \"\"\"\n",
    "    num_edges = edge_index.size(1)\n",
    "    num_val = int(num_edges * val_ratio)\n",
    "    num_test = int(num_edges * test_ratio)\n",
    "    num_train = num_edges - (num_val + num_test)\n",
    "    num_message = int(num_train * message_ratio)\n",
    "\n",
    "    # Randomly shuffle edges\n",
    "    perm = torch.randperm(num_edges)\n",
    "\n",
    "    # Split indices\n",
    "    message_idx = perm[:num_message]\n",
    "    supervision_idx = perm[num_message:num_train]\n",
    "    val_idx = perm[num_train:num_train+num_val]\n",
    "    test_idx = perm[num_train+num_val:]\n",
    "\n",
    "    # Create edge sets\n",
    "    message_edges = edge_index[:, message_idx]\n",
    "    supervision_edges = edge_index[:, supervision_idx]\n",
    "    val_edges = edge_index[:, val_idx]\n",
    "    test_edges = edge_index[:, test_idx]\n",
    "\n",
    "    return message_edges, supervision_edges, val_edges, test_edges\n",
    "\n",
    "def split_edges_by_type(paper_edges, topic_edges, val_ratio=0.1, test_ratio=0.1, message_ratio=0.5):\n",
    "    \"\"\"Split both paper citation edges and topic association edges.\"\"\"\n",
    "\n",
    "    # Split citation edges\n",
    "    paper_splits = split_edges(paper_edges, val_ratio, test_ratio, message_ratio)\n",
    "    paper_message, paper_supervision, paper_val, paper_test = paper_splits\n",
    "\n",
    "    # Split topic edges\n",
    "    topic_splits = split_edges(topic_edges, val_ratio, test_ratio, message_ratio)\n",
    "    topic_message, topic_supervision, topic_val, topic_test = topic_splits\n",
    "\n",
    "    return (paper_message, paper_supervision, paper_val, paper_test,\n",
    "            topic_message, topic_supervision, topic_val, topic_test)\n",
    "\n",
    "def calculate_metrics(pred, target):\n",
    "    \"\"\"Calculate accuracy, precision, recall, and F1 score.\"\"\"\n",
    "    from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "    pred_binary = (pred > 0.5).cpu().numpy()\n",
    "    target = target.cpu().numpy()\n",
    "\n",
    "    accuracy = accuracy_score(target, pred_binary)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(target, pred_binary, average='binary')\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "def train_and_evaluate_multi(model, data, edge_splits, paper_id_to_idx, topic_id_to_idx, epochs=100):\n",
    "    (paper_message, paper_supervision, paper_val, paper_test,\n",
    "     topic_message, topic_supervision, topic_val, topic_test) = edge_splits\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def evaluate(z, pos_edge_set, edge_type=\"citation\", name=\"\"):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # Generate negative edges\n",
    "            neg_edge_set = torch.randint(0,\n",
    "                len(paper_id_to_idx) if edge_type == \"citation\" else len(topic_id_to_idx),\n",
    "                (2, pos_edge_set.size(1)))\n",
    "\n",
    "            # Get predictions\n",
    "            if edge_type == \"citation\":\n",
    "                pos_pred = model.decode_citations(z, pos_edge_set).sigmoid()\n",
    "                neg_pred = model.decode_citations(z, neg_edge_set).sigmoid()\n",
    "            else:\n",
    "                pos_pred = model.decode_topics(z, pos_edge_set).sigmoid()\n",
    "                neg_pred = model.decode_topics(z, neg_edge_set).sigmoid()\n",
    "\n",
    "            # Combine predictions and create labels\n",
    "            pred = torch.cat([pos_pred, neg_pred])\n",
    "            target = torch.cat([torch.ones_like(pos_pred), torch.zeros_like(neg_pred)])\n",
    "\n",
    "            metrics = calculate_metrics(pred, target)\n",
    "\n",
    "            print(f\"\\n{name} {edge_type.capitalize()} Metrics:\")\n",
    "            print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "            print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "            print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "            print(f\"F1: {metrics['f1']:.4f}\")\n",
    "\n",
    "            return metrics\n",
    "\n",
    "    best_val_f1 = {'citation': 0, 'topic': 0}\n",
    "    best_model = None\n",
    "\n",
    "    # Combine message edges for initial embedding\n",
    "    combined_message = torch.cat([paper_message, topic_message], dim=1)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Get node embeddings using message edges\n",
    "        z = model.encode(data.x, combined_message)\n",
    "\n",
    "        # Citation prediction loss\n",
    "        neg_paper_edges = torch.randint(0, len(paper_id_to_idx), (2, paper_supervision.size(1)))\n",
    "        paper_pos_pred = model.decode_citations(z, paper_supervision)\n",
    "        paper_neg_pred = model.decode_citations(z, neg_paper_edges)\n",
    "        paper_loss = criterion(paper_pos_pred, torch.ones_like(paper_pos_pred)) + \\\n",
    "                    criterion(paper_neg_pred, torch.zeros_like(paper_neg_pred))\n",
    "\n",
    "        # Topic prediction loss\n",
    "        neg_topic_edges = torch.randint(0, len(topic_id_to_idx), (2, topic_supervision.size(1)))\n",
    "        topic_pos_pred = model.decode_topics(z, topic_supervision)\n",
    "        topic_neg_pred = model.decode_topics(z, neg_topic_edges)\n",
    "        topic_loss = criterion(topic_pos_pred, torch.ones_like(topic_pos_pred)) + \\\n",
    "                    criterion(topic_neg_pred, torch.zeros_like(topic_neg_pred))\n",
    "\n",
    "        # Combined loss\n",
    "        loss = paper_loss + topic_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"\\nEpoch {epoch+1:03d}:\")\n",
    "\n",
    "            # Evaluate citations\n",
    "            train_citation_metrics = evaluate(z, paper_supervision, \"citation\", \"Training\")\n",
    "            val_citation_metrics = evaluate(z, paper_val, \"citation\", \"Validation\")\n",
    "\n",
    "            # Evaluate topic associations\n",
    "            train_topic_metrics = evaluate(z, topic_supervision, \"topic\", \"Training\")\n",
    "            val_topic_metrics = evaluate(z, topic_val, \"topic\", \"Validation\")\n",
    "\n",
    "            # Save best model based on average F1 score\n",
    "            current_val_f1 = {\n",
    "                'citation': val_citation_metrics['f1'],\n",
    "                'topic': val_topic_metrics['f1']\n",
    "            }\n",
    "\n",
    "            if (current_val_f1['citation'] + current_val_f1['topic'])/2 > \\\n",
    "               (best_val_f1['citation'] + best_val_f1['topic'])/2:\n",
    "                best_val_f1 = current_val_f1\n",
    "                best_model = copy.deepcopy(model)\n",
    "\n",
    "    # Final evaluation on test set\n",
    "    print(\"\\nFinal Test Set Evaluation:\")\n",
    "    z = best_model.encode(data.x, combined_message)\n",
    "    test_citation_metrics = evaluate(z, paper_test, \"citation\", \"Test\")\n",
    "    test_topic_metrics = evaluate(z, topic_test, \"topic\", \"Test\")\n",
    "\n",
    "    return best_model, test_citation_metrics, test_topic_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T-7Jrl6HBKlo",
    "outputId": "004d0898-f66f-404d-b764-e47603f38136"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for https://api.openalex.org/works/W1576013682: 'NoneType' object is not subscriptable\n",
      "Error fetching data for https://api.openalex.org/works/W59369663: 'NoneType' object is not subscriptable\n",
      "Error fetching data for https://api.openalex.org/works/W2778153218: Expecting value: line 1 column 1 (char 0)\n",
      "Error fetching data for https://api.openalex.org/works/W2527526854: Expecting value: line 1 column 1 (char 0)\n",
      "Error fetching data for https://api.openalex.org/works/W170893625: 'NoneType' object is not subscriptable\n",
      "Error fetching data for https://api.openalex.org/works/W2159707944: Expecting value: line 1 column 1 (char 0)\n",
      "Error fetching data for https://api.openalex.org/works/W2332681686: Expecting value: line 1 column 1 (char 0)\n",
      "Error fetching data for https://api.openalex.org/works/W2029411830: Expecting value: line 1 column 1 (char 0)\n",
      "Error fetching data for https://api.openalex.org/works/W1976806156: Expecting value: line 1 column 1 (char 0)\n",
      "Error fetching data for https://api.openalex.org/works/W2060903877: Expecting value: line 1 column 1 (char 0)\n",
      "Error fetching data for https://api.openalex.org/works/W2906871482: 'primary_topic'\n",
      "Error fetching data for https://api.openalex.org/works/W2332681686: Expecting value: line 1 column 1 (char 0)\n",
      "Error fetching data for https://api.openalex.org/works/W2029411830: Expecting value: line 1 column 1 (char 0)\n",
      "Error fetching data for https://api.openalex.org/works/W1976806156: Expecting value: line 1 column 1 (char 0)\n",
      "Error fetching data for https://api.openalex.org/works/W2082559087: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "Error fetching data for https://api.openalex.org/works/W1994880295: max() arg is an empty sequence\n",
      "Error fetching data for https://api.openalex.org/works/W194249466: Expecting value: line 1 column 1 (char 0)\n",
      "Error fetching data for https://api.openalex.org/works/W2022532533: Expecting value: line 1 column 1 (char 0)\n",
      "Error fetching data for https://api.openalex.org/works/W2109255472: Expecting value: line 1 column 1 (char 0)\n",
      "Error fetching data for https://api.openalex.org/works/W4285719527: Expecting value: line 1 column 1 (char 0)\n",
      "Error fetching data for https://api.openalex.org/works/W2592929672: Expecting value: line 1 column 1 (char 0)\n",
      "Error fetching data for https://api.openalex.org/works/W2164578725: max() arg is an empty sequence\n",
      "Error fetching data for https://api.openalex.org/works/W2502651140: Expecting value: line 1 column 1 (char 0)\n",
      "Error fetching data for https://api.openalex.org/works/W2161222316: Expecting value: line 1 column 1 (char 0)\n",
      "Error fetching data for https://api.openalex.org/works/W2045611938: Expecting value: line 1 column 1 (char 0)\n",
      "Error fetching data for https://api.openalex.org/works/W2158714788: Expecting value: line 1 column 1 (char 0)\n",
      "Error fetching data for https://api.openalex.org/works/W2108244474: max() arg is an empty sequence\n",
      "Error fetching data for https://api.openalex.org/works/W2127230663: Expecting value: line 1 column 1 (char 0)\n",
      "Error fetching data for https://api.openalex.org/works/W2134979684: Expecting value: line 1 column 1 (char 0)\n",
      "Error fetching data for https://api.openalex.org/works/W2134759932: Expecting value: line 1 column 1 (char 0)\n",
      "Error fetching data for https://api.openalex.org/works/W2740133093: Expecting value: line 1 column 1 (char 0)\n",
      "Error fetching data for https://api.openalex.org/works/W4285719527: Expecting value: line 1 column 1 (char 0)\n",
      "Error fetching data for https://api.openalex.org/works/W2066848874: Expecting value: line 1 column 1 (char 0)\n",
      "Error fetching data for https://api.openalex.org/works/W1986252415: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n",
      "Error fetching data for https://api.openalex.org/works/W1533335653: max() arg is an empty sequence\n",
      "Error fetching data for https://api.openalex.org/works/W2093898784: max() arg is an empty sequence\n",
      "Error fetching data for https://api.openalex.org/works/W2011215428: max() arg is an empty sequence\n",
      "Error fetching data for https://api.openalex.org/works/W2159707944: Expecting value: line 1 column 1 (char 0)\n",
      "Error fetching data for https://api.openalex.org/works/W2537623931: Expecting value: line 1 column 1 (char 0)\n",
      "Error fetching data for https://api.openalex.org/works/W2151694441: Expecting value: line 1 column 1 (char 0)\n",
      "Error fetching data for https://api.openalex.org/works/W2051969447: Expecting value: line 1 column 1 (char 0)\n",
      "Error fetching data for https://api.openalex.org/works/W2158714788: Expecting value: line 1 column 1 (char 0)\n",
      "Error fetching data for https://api.openalex.org/works/W2127230663: Expecting value: line 1 column 1 (char 0)\n",
      "Error fetching data for https://api.openalex.org/works/W2154431984: Expecting value: line 1 column 1 (char 0)\n",
      "Error fetching data for https://api.openalex.org/works/W2149429041: Expecting value: line 1 column 1 (char 0)\n",
      "Error fetching data for https://api.openalex.org/works/W2134979684: Expecting value: line 1 column 1 (char 0)\n",
      "Error fetching data for https://api.openalex.org/works/W2180229411: Expecting value: line 1 column 1 (char 0)\n",
      "Error fetching data for https://api.openalex.org/works/W2107959600: Expecting value: line 1 column 1 (char 0)\n",
      "Error fetching data for https://api.openalex.org/works/W2134759932: Expecting value: line 1 column 1 (char 0)\n",
      "Error fetching data for https://api.openalex.org/works/W2037210253: Expecting value: line 1 column 1 (char 0)\n",
      "Error fetching data for https://api.openalex.org/works/W3089200134: Expecting value: line 1 column 1 (char 0)\n",
      "Error fetching data for https://api.openalex.org/works/W4285719527: Expecting value: line 1 column 1 (char 0)\n",
      "Error fetching data for https://api.openalex.org/works/W2160815625: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haris\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\utils\\generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Graph Statistics ===\n",
      "\n",
      "Node Counts:\n",
      "Papers: 49749\n",
      "Topics: 2594\n",
      "Total Nodes: 52343\n",
      "\n",
      "Total Edge Counts:\n",
      "Citations (paper → paper): 39346\n",
      "Topic Associations (paper ↔ topic): 98246\n",
      "Total Edges: 137592\n",
      "\n",
      "Citation Edge Splits:\n",
      "Message Edges: 15739\n",
      "Supervision Edges: 15739\n",
      "Validation Edges: 3934\n",
      "Test Edges: 3934\n",
      "Total: 39346\n",
      "\n",
      "Topic Association Edge Splits:\n",
      "Message Edges: 39299\n",
      "Supervision Edges: 39299\n",
      "Validation Edges: 9824\n",
      "Test Edges: 9824\n",
      "Total: 98246\n",
      "\n",
      "Graph Density:\n",
      "Citation Density: 0.000016\n",
      "Topic Association Density: 0.000761\n",
      "\n",
      "Topic Distribution:\n",
      "Top 5 topics by paper count:\n",
      "Topic: Advancements in Density Functional Theory, Papers: 2288\n",
      "Topic: Diagnosis and Management of Alzheimer's Disease, Papers: 941\n",
      "Topic: Methods for Evidence Synthesis in Research, Papers: 831\n",
      "Topic: Molecular Mechanisms of Synaptic Plasticity and Neurological Disorders, Papers: 806\n",
      "Topic: , Papers: 626\n",
      "\n",
      "Epoch 010:\n",
      "\n",
      "Training Citation Metrics:\n",
      "Accuracy: 0.7592\n",
      "Precision: 0.7611\n",
      "Recall: 0.7555\n",
      "F1: 0.7583\n",
      "\n",
      "Validation Citation Metrics:\n",
      "Accuracy: 0.7518\n",
      "Precision: 0.7553\n",
      "Recall: 0.7448\n",
      "F1: 0.7500\n",
      "\n",
      "Training Topic Metrics:\n",
      "Accuracy: 0.9227\n",
      "Precision: 0.9547\n",
      "Recall: 0.8876\n",
      "F1: 0.9199\n",
      "\n",
      "Validation Topic Metrics:\n",
      "Accuracy: 0.9217\n",
      "Precision: 0.9545\n",
      "Recall: 0.8856\n",
      "F1: 0.9187\n",
      "\n",
      "Epoch 020:\n",
      "\n",
      "Training Citation Metrics:\n",
      "Accuracy: 0.8111\n",
      "Precision: 0.7712\n",
      "Recall: 0.8848\n",
      "F1: 0.8241\n",
      "\n",
      "Validation Citation Metrics:\n",
      "Accuracy: 0.8066\n",
      "Precision: 0.7676\n",
      "Recall: 0.8793\n",
      "F1: 0.8197\n",
      "\n",
      "Training Topic Metrics:\n",
      "Accuracy: 0.9599\n",
      "Precision: 0.9756\n",
      "Recall: 0.9434\n",
      "F1: 0.9592\n",
      "\n",
      "Validation Topic Metrics:\n",
      "Accuracy: 0.9578\n",
      "Precision: 0.9758\n",
      "Recall: 0.9389\n",
      "F1: 0.9570\n",
      "\n",
      "Epoch 030:\n",
      "\n",
      "Training Citation Metrics:\n",
      "Accuracy: 0.8692\n",
      "Precision: 0.8398\n",
      "Recall: 0.9124\n",
      "F1: 0.8746\n",
      "\n",
      "Validation Citation Metrics:\n",
      "Accuracy: 0.8607\n",
      "Precision: 0.8312\n",
      "Recall: 0.9052\n",
      "F1: 0.8666\n",
      "\n",
      "Training Topic Metrics:\n",
      "Accuracy: 0.9710\n",
      "Precision: 0.9798\n",
      "Recall: 0.9617\n",
      "F1: 0.9707\n",
      "\n",
      "Validation Topic Metrics:\n",
      "Accuracy: 0.9695\n",
      "Precision: 0.9791\n",
      "Recall: 0.9594\n",
      "F1: 0.9692\n",
      "\n",
      "Epoch 040:\n",
      "\n",
      "Training Citation Metrics:\n",
      "Accuracy: 0.9038\n",
      "Precision: 0.8849\n",
      "Recall: 0.9283\n",
      "F1: 0.9061\n",
      "\n",
      "Validation Citation Metrics:\n",
      "Accuracy: 0.8981\n",
      "Precision: 0.8842\n",
      "Recall: 0.9161\n",
      "F1: 0.8999\n",
      "\n",
      "Training Topic Metrics:\n",
      "Accuracy: 0.9769\n",
      "Precision: 0.9775\n",
      "Recall: 0.9762\n",
      "F1: 0.9768\n",
      "\n",
      "Validation Topic Metrics:\n",
      "Accuracy: 0.9775\n",
      "Precision: 0.9814\n",
      "Recall: 0.9733\n",
      "F1: 0.9774\n",
      "\n",
      "Epoch 050:\n",
      "\n",
      "Training Citation Metrics:\n",
      "Accuracy: 0.9300\n",
      "Precision: 0.9111\n",
      "Recall: 0.9529\n",
      "F1: 0.9316\n",
      "\n",
      "Validation Citation Metrics:\n",
      "Accuracy: 0.9223\n",
      "Precision: 0.9095\n",
      "Recall: 0.9380\n",
      "F1: 0.9235\n",
      "\n",
      "Training Topic Metrics:\n",
      "Accuracy: 0.9823\n",
      "Precision: 0.9909\n",
      "Recall: 0.9735\n",
      "F1: 0.9821\n",
      "\n",
      "Validation Topic Metrics:\n",
      "Accuracy: 0.9807\n",
      "Precision: 0.9908\n",
      "Recall: 0.9704\n",
      "F1: 0.9805\n",
      "\n",
      "Epoch 060:\n",
      "\n",
      "Training Citation Metrics:\n",
      "Accuracy: 0.9326\n",
      "Precision: 0.9228\n",
      "Recall: 0.9442\n",
      "F1: 0.9333\n",
      "\n",
      "Validation Citation Metrics:\n",
      "Accuracy: 0.9309\n",
      "Precision: 0.9287\n",
      "Recall: 0.9334\n",
      "F1: 0.9310\n",
      "\n",
      "Training Topic Metrics:\n",
      "Accuracy: 0.9841\n",
      "Precision: 0.9865\n",
      "Recall: 0.9816\n",
      "F1: 0.9841\n",
      "\n",
      "Validation Topic Metrics:\n",
      "Accuracy: 0.9817\n",
      "Precision: 0.9871\n",
      "Recall: 0.9762\n",
      "F1: 0.9816\n",
      "\n",
      "Epoch 070:\n",
      "\n",
      "Training Citation Metrics:\n",
      "Accuracy: 0.9496\n",
      "Precision: 0.9388\n",
      "Recall: 0.9618\n",
      "F1: 0.9502\n",
      "\n",
      "Validation Citation Metrics:\n",
      "Accuracy: 0.9460\n",
      "Precision: 0.9421\n",
      "Recall: 0.9504\n",
      "F1: 0.9462\n",
      "\n",
      "Training Topic Metrics:\n",
      "Accuracy: 0.9841\n",
      "Precision: 0.9801\n",
      "Recall: 0.9884\n",
      "F1: 0.9842\n",
      "\n",
      "Validation Topic Metrics:\n",
      "Accuracy: 0.9813\n",
      "Precision: 0.9786\n",
      "Recall: 0.9841\n",
      "F1: 0.9814\n",
      "\n",
      "Epoch 080:\n",
      "\n",
      "Training Citation Metrics:\n",
      "Accuracy: 0.9609\n",
      "Precision: 0.9486\n",
      "Recall: 0.9746\n",
      "F1: 0.9614\n",
      "\n",
      "Validation Citation Metrics:\n",
      "Accuracy: 0.9555\n",
      "Precision: 0.9489\n",
      "Recall: 0.9629\n",
      "F1: 0.9558\n",
      "\n",
      "Training Topic Metrics:\n",
      "Accuracy: 0.9885\n",
      "Precision: 0.9957\n",
      "Recall: 0.9811\n",
      "F1: 0.9884\n",
      "\n",
      "Validation Topic Metrics:\n",
      "Accuracy: 0.9851\n",
      "Precision: 0.9956\n",
      "Recall: 0.9746\n",
      "F1: 0.9850\n",
      "\n",
      "Epoch 090:\n",
      "\n",
      "Training Citation Metrics:\n",
      "Accuracy: 0.9666\n",
      "Precision: 0.9538\n",
      "Recall: 0.9808\n",
      "F1: 0.9671\n",
      "\n",
      "Validation Citation Metrics:\n",
      "Accuracy: 0.9629\n",
      "Precision: 0.9580\n",
      "Recall: 0.9682\n",
      "F1: 0.9631\n",
      "\n",
      "Training Topic Metrics:\n",
      "Accuracy: 0.9914\n",
      "Precision: 0.9925\n",
      "Recall: 0.9902\n",
      "F1: 0.9914\n",
      "\n",
      "Validation Topic Metrics:\n",
      "Accuracy: 0.9877\n",
      "Precision: 0.9906\n",
      "Recall: 0.9848\n",
      "F1: 0.9877\n",
      "\n",
      "Epoch 100:\n",
      "\n",
      "Training Citation Metrics:\n",
      "Accuracy: 0.9746\n",
      "Precision: 0.9618\n",
      "Recall: 0.9884\n",
      "F1: 0.9749\n",
      "\n",
      "Validation Citation Metrics:\n",
      "Accuracy: 0.9671\n",
      "Precision: 0.9579\n",
      "Recall: 0.9771\n",
      "F1: 0.9674\n",
      "\n",
      "Training Topic Metrics:\n",
      "Accuracy: 0.9929\n",
      "Precision: 0.9960\n",
      "Recall: 0.9898\n",
      "F1: 0.9929\n",
      "\n",
      "Validation Topic Metrics:\n",
      "Accuracy: 0.9898\n",
      "Precision: 0.9967\n",
      "Recall: 0.9829\n",
      "F1: 0.9897\n",
      "\n",
      "Final Test Set Evaluation:\n",
      "\n",
      "Test Citation Metrics:\n",
      "Accuracy: 0.9692\n",
      "Precision: 0.9638\n",
      "Recall: 0.9751\n",
      "F1: 0.9694\n",
      "\n",
      "Test Topic Metrics:\n",
      "Accuracy: 0.9904\n",
      "Precision: 0.9960\n",
      "Recall: 0.9847\n",
      "F1: 0.9903\n",
      "\n",
      "Final Test Results:\n",
      "\n",
      "Citation Prediction:\n",
      "Accuracy: 0.9692\n",
      "Precision: 0.9638\n",
      "Recall: 0.9751\n",
      "F1: 0.9694\n",
      "\n",
      "Topic Association Prediction:\n",
      "Accuracy: 0.9904\n",
      "Precision: 0.9960\n",
      "Recall: 0.9847\n",
      "F1: 0.9903\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "papers_dict, topics_dict = fetch_openalex_data(degree_limit=2)\n",
    "\n",
    "# Create graph\n",
    "graph_data, paper_id_to_idx, topic_id_to_idx = create_heterogeneous_graph(papers_dict, topics_dict)\n",
    "\n",
    "# Get paper citation edges and topic association edges\n",
    "paper_edges = graph_data['paper', 'cites', 'paper'].edge_index\n",
    "topic_edges = graph_data['paper', 'has_topic', 'topic'].edge_index\n",
    "\n",
    "# Split edges\n",
    "edge_splits = split_edges_by_type(paper_edges, topic_edges)\n",
    "\n",
    "print_graph_statistics(graph_data, papers_dict, topics_dict, edge_splits)\n",
    "\n",
    "# Initialize and train model\n",
    "model = HeteroGCNMultiPredictor(feature_dim=graph_data.x.size(1))\n",
    "best_model, test_citation_metrics, test_topic_metrics = train_and_evaluate_multi(\n",
    "    model, graph_data, edge_splits, paper_id_to_idx, topic_id_to_idx\n",
    ")\n",
    "\n",
    "# Print final results\n",
    "print(\"\\nFinal Test Results:\")\n",
    "print(\"\\nCitation Prediction:\")\n",
    "print(f\"Accuracy: {test_citation_metrics['accuracy']:.4f}\")\n",
    "print(f\"Precision: {test_citation_metrics['precision']:.4f}\")\n",
    "print(f\"Recall: {test_citation_metrics['recall']:.4f}\")\n",
    "print(f\"F1: {test_citation_metrics['f1']:.4f}\")\n",
    "\n",
    "print(\"\\nTopic Association Prediction:\")\n",
    "print(f\"Accuracy: {test_topic_metrics['accuracy']:.4f}\")\n",
    "print(f\"Precision: {test_topic_metrics['precision']:.4f}\")\n",
    "print(f\"Recall: {test_topic_metrics['recall']:.4f}\")\n",
    "print(f\"F1: {test_topic_metrics['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "id": "uyHc8f5lu2GJ",
    "outputId": "cd2760d8-6c92-4e39-ac03-787444fed827"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data from OpenAlex...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-1d8e9f93a721>\u001b[0m in \u001b[0;36m<cell line: 445>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    443\u001b[0m     \u001b[0;31m# print(\"Visualization saved as 'graph_evolution.gif'\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-1d8e9f93a721>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[0;31m# Load data from OpenAlex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Fetching data from OpenAlex...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m     \u001b[0mpapers_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopics_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_openalex_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdegree_limit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;31m# Create graph data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-78a68e3af5a0>\u001b[0m in \u001b[0;36mfetch_openalex_data\u001b[0;34m(paper_limit, degree_limit)\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0mnew_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mref_work\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"openalex.org\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"api.openalex.org/works\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mref_work\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"related_works\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0mcited_by_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"cited_by_api_url\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                 \u001b[0mcited_by_papers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcited_by_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"results\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m                 \u001b[0mnew_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwork\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"openalex.org\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"api.openalex.org/works\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mwork\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcited_by_papers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \"\"\"\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"get\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    587\u001b[0m         }\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m             resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    668\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m             \u001b[0;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             response = self._make_request(\n\u001b[0m\u001b[1;32m    790\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[0;31m# Receive the response from the server\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    537\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m         \u001b[0;31m# Get the response from http.client.HTTPConnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m         \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1301\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1303\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1305\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1157\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1159\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def discover_cross_topic_links(model, data, papers_dict, paper_id_to_idx, threshold=0.8):\n",
    "    \"\"\"\n",
    "    Discovers potential links between papers of different topics.\n",
    "\n",
    "    Args:\n",
    "        model: Trained GNN model\n",
    "        data: Graph data object\n",
    "        papers_dict: Dictionary containing paper information\n",
    "        paper_id_to_idx: Mapping from paper IDs to indices\n",
    "        threshold: Probability threshold for considering a link (default: 0.8)\n",
    "\n",
    "    Returns:\n",
    "        List of dictionaries containing discovered links and their probabilities\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Get node embeddings using all available edges\n",
    "        z = model.encode(data.x, data['paper', 'cites', 'paper'].edge_index)\n",
    "        z_papers = z[:len(paper_id_to_idx)]\n",
    "\n",
    "        # Create reverse mapping from indices to paper IDs\n",
    "        idx_to_paper_id = {idx: pid for pid, idx in paper_id_to_idx.items()}\n",
    "\n",
    "        # Generate all possible pairs of papers from different topics\n",
    "        potential_links = []\n",
    "        paper_indices = list(range(len(paper_id_to_idx)))\n",
    "\n",
    "        for i in range(len(paper_indices)):\n",
    "            for j in range(i + 1, len(paper_indices)):\n",
    "                paper1_id = idx_to_paper_id[paper_indices[i]]\n",
    "                paper2_id = idx_to_paper_id[paper_indices[j]]\n",
    "\n",
    "                # Check if papers are from different topics\n",
    "                if papers_dict[paper1_id]['topic'] != papers_dict[paper2_id]['topic']:\n",
    "                    potential_links.append([paper_indices[i], paper_indices[j]])\n",
    "\n",
    "        if not potential_links:\n",
    "            return []\n",
    "\n",
    "        # Convert to tensor and predict links\n",
    "        potential_edges = torch.tensor(potential_links, dtype=torch.long).t()\n",
    "        predictions = model.decode(z_papers, potential_edges).sigmoid()\n",
    "\n",
    "        # Collect high-probability links\n",
    "        discovered_links = []\n",
    "        for i, prob in enumerate(predictions):\n",
    "            if prob > threshold:\n",
    "                paper1_idx = potential_links[i][0]\n",
    "                paper2_idx = potential_links[i][1]\n",
    "                paper1_id = idx_to_paper_id[paper1_idx]\n",
    "                paper2_id = idx_to_paper_id[paper2_idx]\n",
    "\n",
    "                discovered_links.append({\n",
    "                    'paper1': {\n",
    "                        'id': paper1_id,\n",
    "                        'title': papers_dict[paper1_id]['title'],\n",
    "                        'topic': papers_dict[paper1_id]['topic']\n",
    "                    },\n",
    "                    'paper2': {\n",
    "                        'id': paper2_id,\n",
    "                        'title': papers_dict[paper2_id]['title'],\n",
    "                        'topic': papers_dict[paper2_id]['topic']\n",
    "                    },\n",
    "                    'probability': prob.item()\n",
    "                })\n",
    "\n",
    "        # Sort by probability in descending order\n",
    "        discovered_links.sort(key=lambda x: x['probability'], reverse=True)\n",
    "\n",
    "        return discovered_links\n",
    "\n",
    "def split_edges(edge_index, val_ratio=0.1, test_ratio=0.1, message_ratio=0.5):\n",
    "    \"\"\"\n",
    "    Split edges into training message edges, training supervision edges, validation edges, and test edges.\n",
    "    message_ratio determines split between message and supervision edges within training set.\n",
    "    \"\"\"\n",
    "    num_edges = edge_index.size(1)\n",
    "    num_val = int(num_edges * val_ratio)\n",
    "    num_test = int(num_edges * test_ratio)\n",
    "    num_train = num_edges - (num_val + num_test)\n",
    "    num_message = int(num_train * message_ratio)\n",
    "\n",
    "    # Randomly shuffle edges\n",
    "    perm = torch.randperm(num_edges)\n",
    "\n",
    "    # Split indices\n",
    "    message_idx = perm[:num_message]\n",
    "    supervision_idx = perm[num_message:num_train]\n",
    "    val_idx = perm[num_train:num_train+num_val]\n",
    "    test_idx = perm[num_train+num_val:]\n",
    "\n",
    "    # Create edge sets\n",
    "    message_edges = edge_index[:, message_idx]\n",
    "    supervision_edges = edge_index[:, supervision_idx]\n",
    "    val_edges = edge_index[:, val_idx]\n",
    "    test_edges = edge_index[:, test_idx]\n",
    "\n",
    "    return message_edges, supervision_edges, val_edges, test_edges\n",
    "\n",
    "def calculate_metrics(pred, target):\n",
    "    \"\"\"Calculate accuracy, precision, recall, and F1 score.\"\"\"\n",
    "    from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "    pred_binary = (pred > 0.5).cpu().numpy()\n",
    "    target = target.cpu().numpy()\n",
    "\n",
    "    accuracy = accuracy_score(target, pred_binary)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(target, pred_binary, average='binary')\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "def train_and_evaluate(model, data, message_edges, supervision_edges, val_edges, test_edges, epochs, paper_id_to_idx):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def evaluate(message_edge_set, pos_edge_set, name=\"\"):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            z = model.encode(data.x, message_edge_set)\n",
    "\n",
    "            # Generate negative edges\n",
    "            neg_edge_set = torch.randint(0, len(paper_id_to_idx), (2, pos_edge_set.size(1)))\n",
    "\n",
    "            # Get predictions\n",
    "            pos_pred = model.decode(z, pos_edge_set).sigmoid()\n",
    "            neg_pred = model.decode(z, neg_edge_set).sigmoid()\n",
    "\n",
    "            # Combine predictions and create labels\n",
    "            pred = torch.cat([pos_pred, neg_pred])\n",
    "            target = torch.cat([torch.ones_like(pos_pred), torch.zeros_like(neg_pred)])\n",
    "\n",
    "            # Calculate metrics\n",
    "            metrics = calculate_metrics(pred, target)\n",
    "\n",
    "            print(f\"\\n{name} Metrics:\")\n",
    "            print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "            print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "            print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "            print(f\"F1: {metrics['f1']:.4f}\")\n",
    "\n",
    "            return metrics\n",
    "\n",
    "    best_val_f1 = 0\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Training step using message edges to predict supervision edges\n",
    "        z = model.encode(data.x, message_edges)\n",
    "\n",
    "        # Generate negative edges for training\n",
    "        neg_edges = torch.randint(0, len(paper_id_to_idx), (2, supervision_edges.size(1)))\n",
    "\n",
    "        pos_pred = model.decode(z, supervision_edges)\n",
    "        neg_pred = model.decode(z, neg_edges)\n",
    "\n",
    "        loss = criterion(pos_pred, torch.ones_like(pos_pred)) + criterion(neg_pred, torch.zeros_like(neg_pred))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"\\nEpoch {epoch+1:03d}:\")\n",
    "\n",
    "            # Training evaluation (using message edges to predict supervision edges)\n",
    "            train_metrics = evaluate(message_edges, supervision_edges, \"Training\")\n",
    "\n",
    "            # Validation evaluation (using message + supervision edges to predict validation edges)\n",
    "            combined_train_edges = torch.cat([message_edges, supervision_edges], dim=1)\n",
    "            val_metrics = evaluate(combined_train_edges, val_edges, \"Validation\")\n",
    "\n",
    "            if val_metrics['f1'] > best_val_f1:\n",
    "                best_val_f1 = val_metrics['f1']\n",
    "                best_model = copy.deepcopy(model)\n",
    "\n",
    "    # Final evaluation on test set using best model\n",
    "    print(\"\\nFinal Test Set Evaluation:\")\n",
    "    all_train_edges = torch.cat([message_edges, supervision_edges, val_edges], dim=1)\n",
    "    test_metrics = evaluate(all_train_edges, test_edges, \"Test\")\n",
    "\n",
    "    return best_model, test_metrics\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation, PillowWriter\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def create_visualization(data, papers_dict, topics_dict, message_edges, supervision_edges,\n",
    "                        val_edges, test_edges, model, paper_id_to_idx, topic_id_to_idx,\n",
    "                        threshold=0.7, num_frames=50):\n",
    "    \"\"\"\n",
    "    Creates a GIF showing the evolution of the graph as new edges are predicted.\n",
    "    \"\"\"\n",
    "    # Create reverse mappings\n",
    "    idx_to_paper = {idx: pid for pid, idx in paper_id_to_idx.items()}\n",
    "    idx_to_topic = {idx: tid for tid, idx in topic_id_to_idx.items()}\n",
    "\n",
    "    # Create base graph\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Add paper nodes\n",
    "    for idx, paper_id in idx_to_paper.items():\n",
    "        G.add_node(idx,\n",
    "                  type='paper',\n",
    "                  title=papers_dict[paper_id]['title'],\n",
    "                  topic=papers_dict[paper_id]['topic'])\n",
    "\n",
    "    # Add topic nodes\n",
    "    for idx, topic_id in idx_to_topic.items():\n",
    "        G.add_node(idx + len(paper_id_to_idx),\n",
    "                  type='topic',\n",
    "                  name=topics_dict[topic_id])\n",
    "\n",
    "    # Function to get edge colors based on type\n",
    "    def get_edge_colors(G):\n",
    "        colors = []\n",
    "        for u, v in G.edges():\n",
    "            if G[u][v].get('edge_type') == 'message':\n",
    "                colors.append('blue')\n",
    "            elif G[u][v].get('edge_type') == 'supervision':\n",
    "                colors.append('green')\n",
    "            elif G[u][v].get('edge_type') == 'validation':\n",
    "                colors.append('orange')\n",
    "            elif G[u][v].get('edge_type') == 'test':\n",
    "                colors.append('red')\n",
    "            elif G[u][v].get('edge_type') == 'predicted':\n",
    "                colors.append('purple')\n",
    "            else:\n",
    "                colors.append('gray')\n",
    "        return colors\n",
    "\n",
    "    def get_node_colors(G):\n",
    "        colors = []\n",
    "        for node in G.nodes():\n",
    "            if G.nodes[node]['type'] == 'topic':\n",
    "                colors.append('yellow')\n",
    "            else:\n",
    "                topic = G.nodes[node]['topic']\n",
    "                # Generate a unique color for each topic\n",
    "                hash_val = hash(topic)\n",
    "                r = (hash_val & 0xFF) / 255.0\n",
    "                g = ((hash_val >> 8) & 0xFF) / 255.0\n",
    "                b = ((hash_val >> 16) & 0xFF) / 255.0\n",
    "                colors.append([r, g, b, 0.7])\n",
    "        return colors\n",
    "\n",
    "    # Initialize plot\n",
    "    fig, ax = plt.subplots(figsize=(15, 15))\n",
    "\n",
    "    def update(frame):\n",
    "        ax.clear()\n",
    "\n",
    "        # Add edges progressively\n",
    "        if frame == 0:\n",
    "            # Add message edges\n",
    "            for i in range(message_edges.size(1)):\n",
    "                src, dst = message_edges[:, i].tolist()\n",
    "                G.add_edge(src, dst, edge_type='message')\n",
    "\n",
    "        elif frame == 1:\n",
    "            # Add supervision edges\n",
    "            for i in range(supervision_edges.size(1)):\n",
    "                src, dst = supervision_edges[:, i].tolist()\n",
    "                G.add_edge(src, dst, edge_type='supervision')\n",
    "\n",
    "        elif frame == 2:\n",
    "            # Add validation edges\n",
    "            for i in range(val_edges.size(1)):\n",
    "                src, dst = val_edges[:, i].tolist()\n",
    "                G.add_edge(src, dst, edge_type='validation')\n",
    "\n",
    "        elif frame == 3:\n",
    "            # Add test edges\n",
    "            for i in range(test_edges.size(1)):\n",
    "                src, dst = test_edges[:, i].tolist()\n",
    "                G.add_edge(src, dst, edge_type='test')\n",
    "\n",
    "        else:\n",
    "            # Predict new edges\n",
    "            with torch.no_grad():\n",
    "                z = model.encode(data.x, data['paper', 'cites', 'paper'].edge_index)\n",
    "                z_papers = z[:len(paper_id_to_idx)]\n",
    "\n",
    "                # Sample some random paper pairs\n",
    "                num_predictions = 10\n",
    "                paper_indices = list(range(len(paper_id_to_idx)))\n",
    "                pairs = []\n",
    "                for _ in range(num_predictions):\n",
    "                    i, j = np.random.choice(paper_indices, 2, replace=False)\n",
    "                    pairs.append([i, j])\n",
    "\n",
    "                pairs = torch.tensor(pairs, dtype=torch.long).t()\n",
    "                predictions = model.decode(z_papers, pairs).sigmoid()\n",
    "\n",
    "                # Add high probability edges\n",
    "                for i, prob in enumerate(predictions):\n",
    "                    if prob > threshold:\n",
    "                        src, dst = pairs[:, i].tolist()\n",
    "                        G.add_edge(src, dst, edge_type='predicted')\n",
    "\n",
    "        # Draw the graph\n",
    "        pos = nx.spring_layout(G, k=1/np.sqrt(G.number_of_nodes()), iterations=50)\n",
    "\n",
    "        # Draw nodes\n",
    "        nx.draw_networkx_nodes(G, pos,\n",
    "                             node_color=get_node_colors(G),\n",
    "                             node_size=100)\n",
    "\n",
    "        # Draw edges\n",
    "        nx.draw_networkx_edges(G, pos,\n",
    "                             edge_color=get_edge_colors(G),\n",
    "                             width=1.0,\n",
    "                             alpha=0.5)\n",
    "\n",
    "        # Add title\n",
    "        if frame == 0:\n",
    "            plt.title(\"Message Edges (Training)\", fontsize=16)\n",
    "        elif frame == 1:\n",
    "            plt.title(\"Supervision Edges Added\", fontsize=16)\n",
    "        elif frame == 2:\n",
    "            plt.title(\"Validation Edges Added\", fontsize=16)\n",
    "        elif frame == 3:\n",
    "            plt.title(\"Test Edges Added\", fontsize=16)\n",
    "        else:\n",
    "            plt.title(f\"Predicted Edges (Frame {frame})\", fontsize=16)\n",
    "\n",
    "        # Add legend\n",
    "        legend_elements = [\n",
    "            plt.Line2D([0], [0], color='blue', label='Message'),\n",
    "            plt.Line2D([0], [0], color='green', label='Supervision'),\n",
    "            plt.Line2D([0], [0], color='orange', label='Validation'),\n",
    "            plt.Line2D([0], [0], color='red', label='Test'),\n",
    "            plt.Line2D([0], [0], color='purple', label='Predicted')\n",
    "        ]\n",
    "        ax.legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "        plt.axis('off')\n",
    "\n",
    "    # Create animation\n",
    "    anim = FuncAnimation(fig, update, frames=num_frames, interval=500, repeat=True)\n",
    "\n",
    "    # Save as GIF\n",
    "    writer = PillowWriter(fps=2)\n",
    "    anim.save('graph_evolution.gif', writer=writer)\n",
    "    plt.close()\n",
    "\n",
    "def main():\n",
    "    # Load data from OpenAlex\n",
    "    print(\"Fetching data from OpenAlex...\")\n",
    "    papers_dict, topics_dict = fetch_openalex_data(degree_limit=6)\n",
    "\n",
    "    # Create graph data\n",
    "    print(\"Creating heterogeneous graph...\")\n",
    "    graph_data, paper_id_to_idx, topic_id_to_idx = create_heterogeneous_graph(papers_dict, topics_dict)\n",
    "\n",
    "    # Ensure we have enough edges for meaningful splits\n",
    "    edge_index = graph_data['paper', 'cites', 'paper'].edge_index\n",
    "    num_edges = edge_index.size(1)\n",
    "\n",
    "    if num_edges < 20:  # Minimum threshold for meaningful splitting\n",
    "        print(f\"Warning: Only {num_edges} edges found. Need more data for meaningful evaluation.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Total number of papers: {len(papers_dict)}\")\n",
    "    print(f\"Total number of topics: {len(topics_dict)}\")\n",
    "    print(f\"Total number of citation edges: {num_edges}\")\n",
    "\n",
    "    # Split edges\n",
    "    print(\"\\nSplitting edges...\")\n",
    "    message_edges, supervision_edges, val_edges, test_edges = split_edges(\n",
    "        edge_index,\n",
    "        val_ratio=0.1,\n",
    "        test_ratio=0.1,\n",
    "        message_ratio=0.5\n",
    "    )\n",
    "\n",
    "    print(f\"Number of message edges: {message_edges.size(1)}\")\n",
    "    print(f\"Number of supervision edges: {supervision_edges.size(1)}\")\n",
    "    print(f\"Number of validation edges: {val_edges.size(1)}\")\n",
    "    print(f\"Number of test edges: {test_edges.size(1)}\")\n",
    "\n",
    "    # Initialize model\n",
    "    print(\"\\nInitializing model...\")\n",
    "    model = HeteroGCNLinkPredictor(\n",
    "        feature_dim=graph_data.x.size(1),\n",
    "        hidden_channels=64\n",
    "    )\n",
    "\n",
    "    # Train and evaluate model\n",
    "    print(\"\\nStarting training and evaluation...\")\n",
    "    best_model, test_metrics = train_and_evaluate(\n",
    "        model=model,\n",
    "        data=graph_data,\n",
    "        message_edges=message_edges,\n",
    "        supervision_edges=supervision_edges,\n",
    "        val_edges=val_edges,\n",
    "        test_edges=test_edges,\n",
    "        epochs=100,\n",
    "        paper_id_to_idx=paper_id_to_idx\n",
    "    )\n",
    "\n",
    "    # Print final test metrics\n",
    "    print(\"\\nFinal Test Set Performance:\")\n",
    "    print(f\"Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {test_metrics['precision']:.4f}\")\n",
    "    print(f\"Recall: {test_metrics['recall']:.4f}\")\n",
    "    print(f\"F1: {test_metrics['f1']:.4f}\")\n",
    "\n",
    "    # Optionally: Use model to predict new cross-topic links\n",
    "    print(\"\\nDiscovering potential cross-topic links...\")\n",
    "    discovered_links = discover_cross_topic_links(\n",
    "        best_model,\n",
    "        graph_data,\n",
    "        papers_dict,\n",
    "        paper_id_to_idx,\n",
    "        threshold=0.8\n",
    "    )\n",
    "\n",
    "    # Print top discovered links\n",
    "    if discovered_links:\n",
    "        print(\"\\nTop 5 Discovered Cross-Topic Links:\")\n",
    "        for i, link in enumerate(discovered_links[:5], 1):\n",
    "            print(f\"\\n{i}. Probability: {link['probability']:.4f}\")\n",
    "            print(f\"Paper 1 ({topics_dict[link['paper1']['topic']]}): {link['paper1']['title']}\")\n",
    "            print(f\"Paper 2 ({topics_dict[link['paper2']['topic']]}): {link['paper2']['title']}\")\n",
    "    else:\n",
    "        print(\"No high-probability cross-topic links discovered.\")\n",
    "\n",
    "    # print(\"\\nGenerating visualization...\")\n",
    "    # create_visualization(\n",
    "    #     graph_data, papers_dict, topics_dict,\n",
    "    #     message_edges, supervision_edges, val_edges, test_edges,\n",
    "    #     best_model, paper_id_to_idx, topic_id_to_idx\n",
    "    # )\n",
    "    # print(\"Visualization saved as 'graph_evolution.gif'\")\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RrgKhHujqRfW"
   },
   "outputs": [],
   "source": [
    "# graph_data, paper_id_to_idx, topic_id_to_idx = create_heterogeneous_graph(papers_dict, topics_dict)\n",
    "\n",
    "# # Train models for same-topic and different-topic link prediction\n",
    "# same_topic_model = train_link_prediction(graph_data, papers_dict, paper_id_to_idx, mode='same_topic')\n",
    "# different_topic_model = train_link_prediction(graph_data, papers_dict, paper_id_to_idx, mode='different_topic')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VB_QguF6u6ny"
   },
   "outputs": [],
   "source": [
    "\n",
    "# graph_data, paper_id_to_idx, topic_id_to_idx = create_heterogeneous_graph(papers_dict, topics_dict)\n",
    "\n",
    "# # Train models\n",
    "# different_topic_model, train_metrics, val_metrics = train_link_prediction(\n",
    "#     graph_data, papers_dict, paper_id_to_idx, mode='different_topic'\n",
    "# )\n",
    "\n",
    "# # Discover new cross-topic links\n",
    "# discovered_links = discover_cross_topic_links(\n",
    "#     different_topic_model, graph_data, papers_dict, paper_id_to_idx\n",
    "# )\n",
    "\n",
    "# print(\"\\nTop 5 Discovered Cross-Topic Links:\")\n",
    "# for link in discovered_links[:5]:\n",
    "#     print(f\"\\nProbability: {link['probability']:.4f}\")\n",
    "#     print(f\"Paper 1 ({link['paper1']['topic']}): {link['paper1']['title']}\")\n",
    "#     print(f\"Paper 2 ({link['paper2']['topic']}): {link['paper2']['title']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d2queHwfqq27"
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.animation import FuncAnimation, PillowWriter\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def create_initial_graph():\n",
    "    \"\"\"Create initial heterogeneous graph with two node types and edge types\"\"\"\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Add nodes of type 1 (e.g., users)\n",
    "    for i in range(5):\n",
    "        G.add_node(f'U{i}', type='user', pos=(np.random.random(), np.random.random()))\n",
    "\n",
    "    # Add nodes of type 2 (e.g., items)\n",
    "    for i in range(5):\n",
    "        G.add_node(f'I{i}', type='item', pos=(np.random.random(), np.random.random()))\n",
    "\n",
    "    # Add initial edges of type 1 (e.g., user-item interactions)\n",
    "    initial_edges = [\n",
    "        ('U0', 'I1'), ('U1', 'I2'), ('U2', 'I0'),\n",
    "        ('U3', 'I3'), ('U4', 'I4')\n",
    "    ]\n",
    "    G.add_edges_from(initial_edges, type='interaction')\n",
    "\n",
    "    # Add initial edges of type 2 (e.g., user-user relationships)\n",
    "    user_edges = [\n",
    "        ('U0', 'U1'), ('U2', 'U3'), ('U3', 'U4')\n",
    "    ]\n",
    "    G.add_edges_from(user_edges, type='friendship')\n",
    "\n",
    "    return G\n",
    "\n",
    "def predict_new_edges(G, frame):\n",
    "    \"\"\"Simulate edge prediction by adding new edges based on frame number\"\"\"\n",
    "    predictions = [\n",
    "        ('U0', 'I3', 'interaction'),\n",
    "        ('U1', 'I4', 'interaction'),\n",
    "        ('U1', 'U3', 'friendship'),\n",
    "        ('U2', 'I4', 'interaction'),\n",
    "        ('U0', 'U4', 'friendship')\n",
    "    ]\n",
    "\n",
    "    if frame < len(predictions):\n",
    "        new_edge = predictions[frame]\n",
    "        G.add_edge(new_edge[0], new_edge[1], type=new_edge[2])\n",
    "\n",
    "    return G\n",
    "\n",
    "def update(frame, G, ax):\n",
    "    \"\"\"Update function for animation\"\"\"\n",
    "    ax.clear()\n",
    "\n",
    "    # Update graph with new predicted edges\n",
    "    G = predict_new_edges(G, frame)\n",
    "\n",
    "    # Get node positions\n",
    "    pos = nx.get_node_attributes(G, 'pos')\n",
    "\n",
    "    # Draw nodes by type\n",
    "    user_nodes = [node for node, attr in G.nodes(data=True) if attr['type'] == 'user']\n",
    "    item_nodes = [node for node, attr in G.nodes(data=True) if attr['type'] == 'item']\n",
    "\n",
    "    nx.draw_networkx_nodes(G, pos, nodelist=user_nodes, node_color='lightblue',\n",
    "                          node_size=500)\n",
    "    nx.draw_networkx_nodes(G, pos, nodelist=item_nodes, node_color='lightgreen',\n",
    "                          node_shape='s', node_size=500)\n",
    "\n",
    "    # Draw edges by type\n",
    "    interaction_edges = [(u, v) for (u, v, d) in G.edges(data=True) if d['type'] == 'interaction']\n",
    "    friendship_edges = [(u, v) for (u, v, d) in G.edges(data=True) if d['type'] == 'friendship']\n",
    "\n",
    "    nx.draw_networkx_edges(G, pos, edgelist=interaction_edges, edge_color='red',\n",
    "                          width=2)\n",
    "    nx.draw_networkx_edges(G, pos, edgelist=friendship_edges, edge_color='blue',\n",
    "                          style='dashed', width=2)\n",
    "\n",
    "    # Draw labels\n",
    "    nx.draw_networkx_labels(G, pos)\n",
    "\n",
    "    # Add title and legend\n",
    "    # ax.set_title(f'Heterogeneous Graph Evolution (Step {frame+1})')\n",
    "    # ax.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "    # Set limits and remove axes\n",
    "    ax.set_xlim(-0.1, 1.1)\n",
    "    ax.set_ylim(-0.1, 1.1)\n",
    "    ax.axis('off')\n",
    "\n",
    "def create_graph_animation():\n",
    "    \"\"\"Create and save the graph animation as a GIF\"\"\"\n",
    "    # Create initial graph\n",
    "    G = create_initial_graph()\n",
    "\n",
    "    # Set up the figure\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    plt.subplots_adjust(right=0.85)\n",
    "\n",
    "    # Create animation\n",
    "    frames = 6  # Number of prediction steps + 1\n",
    "    anim = FuncAnimation(fig, update, frames=frames, fargs=(G, ax),\n",
    "                        interval=1000, repeat=True)\n",
    "\n",
    "    # Save as GIF\n",
    "    writer = PillowWriter(fps=1)\n",
    "    anim.save('hetero_graph_evolution.gif', writer=writer)\n",
    "    plt.close()\n",
    "\n",
    "# Generate the animation\n",
    "create_graph_animation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NrvaeTEqQFxP"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
