{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ML with Graphs\n",
    "\n",
    "## Cross Domain Knowledge Discovery\n",
    "\n",
    "- Harish Varma Siravuri\n",
    "- Saurav Mukhopadhyay"
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T01:13:25.166871Z",
     "start_time": "2024-11-26T01:13:23.271920Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.nn import GCNConv, SAGEConv\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm.notebook import tqdm\n",
    "from torch_geometric.nn import HeteroConv, SAGEConv, Linear\n",
    "from torch_geometric.loader import NeighborLoader"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T01:13:43.961343Z",
     "start_time": "2024-11-26T01:13:43.941580Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_abstract(inverted_index):\n",
    "    if inverted_index is not None:\n",
    "        abstract_words = []\n",
    "        max_index = max(max(positions) for positions in inverted_index.values())\n",
    "        abstract_words = [None] * (max_index + 1)\n",
    "        for word, positions in inverted_index.items():\n",
    "            for position in positions:\n",
    "                abstract_words[position] = word\n",
    "\n",
    "        abstract = \" \".join(\n",
    "            [word if word is not None else \"\" for word in abstract_words]\n",
    "        )\n",
    "        return abstract\n",
    "    else:\n",
    "        return \"Unavailable\"\n",
    "\n",
    "\n",
    "def fetch_openalex_data(paper_limit: int = 300, degree_limit: int = 1):\n",
    "    base_url = \"https://api.openalex.org/works\"\n",
    "    response = requests.get(base_url)\n",
    "\n",
    "    seed_works = response.json().get(\"results\")\n",
    "    ids = [\n",
    "        work[\"id\"].replace(\"openalex.org\", \"api.openalex.org/works\")\n",
    "        for work in seed_works\n",
    "    ]\n",
    "\n",
    "    # Remove after testing\n",
    "    # ids = [ids[0]]\n",
    "\n",
    "    papers_dict = {}\n",
    "    topics_dict = {}\n",
    "    degree = 0\n",
    "\n",
    "    while degree < degree_limit:\n",
    "        degree += 1\n",
    "        new_ids = []\n",
    "\n",
    "        for pid in ids:\n",
    "            try:\n",
    "                response = requests.get(pid).json()\n",
    "                new_ids.append(response[\"id\"].replace(\"openalex.org\", \"api.openalex.org/works\"))\n",
    "                new_ids.extend([ref_work.replace(\"openalex.org\", \"api.openalex.org/works\") for ref_work in response[\"referenced_works\"]])\n",
    "                new_ids.extend([ref_work.replace(\"openalex.org\", \"api.openalex.org/works\") for ref_work in response[\"related_works\"]])\n",
    "                cited_by_url = response[\"cited_by_api_url\"]\n",
    "                cited_by_papers = requests.get(cited_by_url).json().get(\"results\")\n",
    "                new_ids.extend([work[\"id\"].replace(\"openalex.org\", \"api.openalex.org/works\") for work in cited_by_papers])\n",
    "\n",
    "                dic_topic = response.get('primary_topic')\n",
    "                dic_title = response.get(\"title\")\n",
    "                dic_abstract = response.get(\"abstract_inverted_index\")\n",
    "                topic = \"\"\n",
    "                title = \"\"\n",
    "                abstract = \"\"\n",
    "                if dic_topic is not None:\n",
    "                    topic = dic_topic.get(\"id\", \"\")\n",
    "                if dic_title is not None:\n",
    "                    title = dic_title\n",
    "                if dic_abstract is not None:\n",
    "                    abstract = get_abstract(dic_abstract)\n",
    "\n",
    "                papers_dict[pid] = {\n",
    "                    \"id\": pid,\n",
    "                    \"citation_count\": len(cited_by_papers),\n",
    "                    \"topic\": topic,\n",
    "                    \"title\": title,\n",
    "                    \"abstract\": abstract,\n",
    "                    \"cites\": [ref_work.replace(\"openalex.org\", \"api.openalex.org/works\") for ref_work in response[\"referenced_works\"]]\n",
    "                }\n",
    "                if degree == degree_limit:\n",
    "                    for work in [ref_work.replace(\"openalex.org\", \"api.openalex.org/works\") for ref_work in response[\"referenced_works\"]]:\n",
    "                        res = requests.get(work).json()\n",
    "                        if res is not None:\n",
    "                            dic_topic = res.get('primary_topic')\n",
    "                            dic_title = res.get(\"title\")\n",
    "                            dic_abstract = res.get(\"abstract_inverted_index\")\n",
    "                            topic = \"\"\n",
    "                            title = \"\"\n",
    "                            abstract = \"\"\n",
    "                            if dic_topic is not None:\n",
    "                                topic = dic_topic.get(\"id\", \"\")\n",
    "                            if dic_title is not None:\n",
    "                                title = dic_title\n",
    "                            if dic_abstract is not None:\n",
    "                                abstract = get_abstract(dic_abstract)\n",
    "                            papers_dict[work] = {\n",
    "                                \"id\": work,\n",
    "                                \"citation_count\": 0,\n",
    "                                \"topic\": topic,\n",
    "                                \"title\": title,\n",
    "                                \"abstract\": abstract,\n",
    "                                \"cites\": []\n",
    "                            }\n",
    "                            if dic_topic is not None:\n",
    "                                topics_dict[res[\"primary_topic\"][\"id\"]] = res[\"primary_topic\"][\"display_name\"]\n",
    "                            else:\n",
    "                                topics_dict[\"\"] = \"\"\n",
    "                if topic == \"\":\n",
    "                    topics_dict[\"\"] = \"\"\n",
    "                else:\n",
    "                    topics_dict[response[\"primary_topic\"][\"id\"]] = response[\"primary_topic\"][\"display_name\"]\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching data for {pid}: {e}\")\n",
    "                continue\n",
    "\n",
    "        ids = new_ids\n",
    "    return papers_dict, topics_dict"
   ],
   "id": "833ee08439e9a2eb",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T01:16:05.753619Z",
     "start_time": "2024-11-26T01:13:52.050554Z"
    }
   },
   "cell_type": "code",
   "source": "papers_dict, topics_dict = fetch_openalex_data(degree_limit=1)",
   "id": "c1bdf18a84a8d89b",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T01:16:07.005018Z",
     "start_time": "2024-11-26T01:16:05.859611Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.nn import GCNConv, SAGEConv\n",
    "import torch.nn.functional as F\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "def create_heterogeneous_graph(papers_dict, topics_dict):\n",
    "    # Collect unique topics\n",
    "    unique_topics = set(paper_info['topic'] for paper_info in papers_dict.values() if paper_info['topic'])\n",
    "\n",
    "    # Create mappings\n",
    "    paper_ids = list(papers_dict.keys())\n",
    "    topic_ids = list(unique_topics)\n",
    "\n",
    "    # Create index mappings\n",
    "    paper_id_to_idx = {pid: idx for idx, pid in enumerate(paper_ids)}\n",
    "    topic_id_to_idx = {tid: idx + len(paper_ids) for idx, tid in enumerate(topic_ids)}\n",
    "\n",
    "    # Use SentenceTransformer for text embeddings\n",
    "    text_embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "    # Prepare node features\n",
    "    total_nodes = len(paper_ids) + len(topic_ids)\n",
    "    feature_dim = 384  # SentenceTransformer embedding dimension\n",
    "    x = torch.zeros(total_nodes, feature_dim)\n",
    "\n",
    "    # Populate paper features\n",
    "    for paper_id in paper_ids:\n",
    "        paper_info = papers_dict[paper_id]\n",
    "        text = f\"{paper_info['title']} {paper_info['abstract']}\"\n",
    "        embedding = text_embedder.encode(text, convert_to_tensor=True)\n",
    "        x[paper_id_to_idx[paper_id]] = embedding\n",
    "\n",
    "    # Populate topic features\n",
    "    for topic_id in topic_ids:\n",
    "        topic_name = topics_dict.get(topic_id, topic_id)\n",
    "        embedding = text_embedder.encode(topic_name, convert_to_tensor=True)\n",
    "        x[topic_id_to_idx[topic_id]] = embedding\n",
    "\n",
    "    # Prepare edge indices\n",
    "    citation_src, citation_dst = [], []\n",
    "    topic_paper_src, topic_paper_dst = [], []\n",
    "\n",
    "    # Create citation edges\n",
    "    for paper_id in paper_ids:\n",
    "        paper_info = papers_dict[paper_id]\n",
    "        src_idx = paper_id_to_idx[paper_id]\n",
    "        for cited_paper in paper_info['cites']:\n",
    "            if cited_paper in paper_id_to_idx:\n",
    "                dst_idx = paper_id_to_idx[cited_paper]\n",
    "                citation_src.append(src_idx)\n",
    "                citation_dst.append(dst_idx)\n",
    "\n",
    "    # Create topic-paper edges\n",
    "    for paper_id in paper_ids:\n",
    "        paper_info = papers_dict[paper_id]\n",
    "        if paper_info['topic'] in topic_id_to_idx:\n",
    "            paper_idx = paper_id_to_idx[paper_id]\n",
    "            topic_idx = topic_id_to_idx[paper_info['topic']]\n",
    "\n",
    "            # Bidirectional edges\n",
    "            topic_paper_src.append(paper_idx)\n",
    "            topic_paper_dst.append(topic_idx)\n",
    "            topic_paper_src.append(topic_idx)\n",
    "            topic_paper_dst.append(paper_idx)\n",
    "\n",
    "    # Create edge indices\n",
    "    citation_edge_index = torch.tensor([citation_src, citation_dst], dtype=torch.long)\n",
    "    topic_paper_edge_index = torch.tensor([topic_paper_src, topic_paper_dst], dtype=torch.long)\n",
    "\n",
    "    # Create HeteroData\n",
    "    data = HeteroData()\n",
    "    data['paper', 'cites', 'paper'].edge_index = citation_edge_index\n",
    "    data['paper', 'has_topic', 'topic'].edge_index = topic_paper_edge_index\n",
    "    data['topic', 'has_paper', 'paper'].edge_index = topic_paper_edge_index.flip(0)\n",
    "    data.x = x\n",
    "\n",
    "    return data, paper_id_to_idx, topic_id_to_idx\n",
    "\n",
    "class HeteroGCNLinkPredictor(torch.nn.Module):\n",
    "    def __init__(self, feature_dim, hidden_channels=64):\n",
    "        super().__init__()\n",
    "\n",
    "        # Graph convolution layers\n",
    "        self.conv1 = GCNConv(feature_dim, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "\n",
    "        # Link prediction MLP\n",
    "        self.link_predictor = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_channels * 2, hidden_channels),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_channels, 1)\n",
    "        )\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        # Graph convolution\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "    def decode(self, z, edge_label_index):\n",
    "        row, col = edge_label_index\n",
    "        z_src = z[row]\n",
    "        z_dst = z[col]\n",
    "\n",
    "        # Concatenate source and destination node embeddings\n",
    "        z_combined = torch.cat([z_src, z_dst], dim=-1)\n",
    "\n",
    "        # Predict link probability\n",
    "        return self.link_predictor(z_combined).squeeze()\n",
    "\n",
    "def train_link_prediction(data, papers_dict, paper_id_to_idx, mode='same_topic', epochs=100):\n",
    "    # Prepare edges based on mode\n",
    "    link_edges = []\n",
    "    for paper1_id, paper1_info in papers_dict.items():\n",
    "        for paper2_id, paper2_info in papers_dict.items():\n",
    "            if paper1_id != paper2_id and paper1_id in paper_id_to_idx and paper2_id in paper_id_to_idx:\n",
    "                # Same topic link prediction\n",
    "                if mode == 'same_topic' and paper1_info['topic'] == paper2_info['topic']:\n",
    "                    link_edges.append([\n",
    "                        paper_id_to_idx[paper1_id],\n",
    "                        paper_id_to_idx[paper2_id]\n",
    "                    ])\n",
    "                # Different topic link prediction\n",
    "                elif mode == 'different_topic' and paper1_info['topic'] != paper2_info['topic']:\n",
    "                    link_edges.append([\n",
    "                        paper_id_to_idx[paper1_id],\n",
    "                        paper_id_to_idx[paper2_id]\n",
    "                    ])\n",
    "\n",
    "    # Check if we have enough edges\n",
    "    if not link_edges:\n",
    "        print(f\"No {mode} edges found. Skipping training.\")\n",
    "        return None\n",
    "\n",
    "    # Convert to tensor\n",
    "    edge_index = torch.tensor(link_edges).t()\n",
    "\n",
    "    # Prepare data splits\n",
    "    num_val = max(int(0.1 * len(link_edges)), 1)\n",
    "    num_test = max(int(0.1 * len(link_edges)), 1)\n",
    "\n",
    "    # Split edges\n",
    "    perm = torch.randperm(edge_index.size(1))\n",
    "    train_idx = perm[num_val + num_test:]\n",
    "    val_idx = perm[:num_val]\n",
    "    test_idx = perm[num_val:num_val+num_test]\n",
    "\n",
    "    train_edge_index = edge_index[:, train_idx]\n",
    "\n",
    "    # Initialize model\n",
    "    model = HeteroGCNLinkPredictor(feature_dim=data.x.size(1))\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        z = model.encode(data.x, data['paper', 'cites', 'paper'].edge_index)\n",
    "\n",
    "        # Restrict z to paper nodes (first len(paper_ids) nodes)\n",
    "        z_papers = z[:len(paper_id_to_idx)]\n",
    "\n",
    "        # Generate negative samples (random sampling)\n",
    "        neg_edge_index = torch.randint(0, len(paper_id_to_idx), (2, train_edge_index.size(1)))\n",
    "\n",
    "        # Positive and negative link predictions\n",
    "        pos_link_pred = model.decode(z_papers, train_edge_index)\n",
    "        neg_link_pred = model.decode(z_papers, neg_edge_index)\n",
    "\n",
    "        # Compute loss\n",
    "        pos_loss = criterion(pos_link_pred, torch.ones_like(pos_link_pred))\n",
    "        neg_loss = criterion(neg_link_pred, torch.zeros_like(neg_link_pred))\n",
    "        loss = pos_loss + neg_loss\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'{mode.capitalize()} Topic Link Prediction - Epoch {epoch+1:03d}, Loss: {loss:.4f}')\n",
    "\n",
    "    return model"
   ],
   "id": "2a1541741a4a160a",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T01:16:33.188658Z",
     "start_time": "2024-11-26T01:16:07.006829Z"
    }
   },
   "cell_type": "code",
   "source": [
    "graph_data, paper_id_to_idx, topic_id_to_idx = create_heterogeneous_graph(papers_dict, topics_dict)\n",
    "\n",
    "# Train models for same-topic and different-topic link prediction\n",
    "same_topic_model = train_link_prediction(graph_data, papers_dict, paper_id_to_idx, mode='same_topic')\n",
    "different_topic_model = train_link_prediction(graph_data, papers_dict, paper_id_to_idx, mode='different_topic')\n"
   ],
   "id": "5cfcb81d5ea2fc72",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'torch._C.Tag' has no attribute 'pt2_compliant_tag'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m graph_data, paper_id_to_idx, topic_id_to_idx \u001B[38;5;241m=\u001B[39m create_heterogeneous_graph(papers_dict, topics_dict)\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# Train models for same-topic and different-topic link prediction\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m same_topic_model \u001B[38;5;241m=\u001B[39m train_link_prediction(graph_data, papers_dict, paper_id_to_idx, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msame_topic\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      5\u001B[0m different_topic_model \u001B[38;5;241m=\u001B[39m train_link_prediction(graph_data, papers_dict, paper_id_to_idx, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdifferent_topic\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "Cell \u001B[1;32mIn[6], line 156\u001B[0m, in \u001B[0;36mtrain_link_prediction\u001B[1;34m(data, papers_dict, paper_id_to_idx, mode, epochs)\u001B[0m\n\u001B[0;32m    153\u001B[0m \u001B[38;5;66;03m# Initialize model\u001B[39;00m\n\u001B[0;32m    154\u001B[0m model \u001B[38;5;241m=\u001B[39m HeteroGCNLinkPredictor(feature_dim\u001B[38;5;241m=\u001B[39mdata\u001B[38;5;241m.\u001B[39mx\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m1\u001B[39m))\n\u001B[1;32m--> 156\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mAdam(model\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.01\u001B[39m)\n\u001B[0;32m    157\u001B[0m criterion \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mBCEWithLogitsLoss()\n\u001B[0;32m    159\u001B[0m \u001B[38;5;66;03m# Training loop\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\optim\\adam.py:45\u001B[0m, in \u001B[0;36mAdam.__init__\u001B[1;34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused)\u001B[0m\n\u001B[0;32m     39\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid weight_decay value: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mweight_decay\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     41\u001B[0m defaults \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m(lr\u001B[38;5;241m=\u001B[39mlr, betas\u001B[38;5;241m=\u001B[39mbetas, eps\u001B[38;5;241m=\u001B[39meps,\n\u001B[0;32m     42\u001B[0m                 weight_decay\u001B[38;5;241m=\u001B[39mweight_decay, amsgrad\u001B[38;5;241m=\u001B[39mamsgrad,\n\u001B[0;32m     43\u001B[0m                 maximize\u001B[38;5;241m=\u001B[39mmaximize, foreach\u001B[38;5;241m=\u001B[39mforeach, capturable\u001B[38;5;241m=\u001B[39mcapturable,\n\u001B[0;32m     44\u001B[0m                 differentiable\u001B[38;5;241m=\u001B[39mdifferentiable, fused\u001B[38;5;241m=\u001B[39mfused)\n\u001B[1;32m---> 45\u001B[0m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(params, defaults)\n\u001B[0;32m     47\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m fused:\n\u001B[0;32m     48\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m differentiable:\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\optim\\optimizer.py:266\u001B[0m, in \u001B[0;36m__init__\u001B[1;34m(self, params, defaults)\u001B[0m\n\u001B[0;32m    262\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(params, torch\u001B[38;5;241m.\u001B[39mTensor):\n\u001B[0;32m    263\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSparseAdam\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m    264\u001B[0m         warnings\u001B[38;5;241m.\u001B[39mwarn((\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPassing in a raw Tensor as ``params`` to SparseAdam \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    265\u001B[0m                        \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mis deprecated. In the future, this will raise an error. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m--> 266\u001B[0m                        \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease wrap your Tensor in an iterable instead.\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m    267\u001B[0m                       \u001B[38;5;167;01mFutureWarning\u001B[39;00m)\n\u001B[0;32m    268\u001B[0m         params \u001B[38;5;241m=\u001B[39m [params]\n\u001B[0;32m    269\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_compile.py:22\u001B[0m, in \u001B[0;36m_disable_dynamo.<locals>.inner\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(fn)\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minner\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m---> 22\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_dynamo\u001B[39;00m\n\u001B[0;32m     24\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_dynamo\u001B[38;5;241m.\u001B[39mdisable(fn, recursive)(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\__init__.py:2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m convert_frame, eval_frame, resume_execution\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbackends\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mregistry\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m list_backends, lookup_backend, register_backend\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcallback\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m callback_handler, on_compile_end, on_compile_start\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\convert_frame.py:40\u001B[0m\n\u001B[0;32m     37\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_python_dispatch\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _disable_current_modes\n\u001B[0;32m     38\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_traceback\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m format_traceback_short\n\u001B[1;32m---> 40\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m config, exc, trace_rules\n\u001B[0;32m     41\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbackends\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mregistry\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m CompilerFn\n\u001B[0;32m     42\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbytecode_analysis\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m remove_dead_code, remove_pointless_jumps\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_dynamo\\trace_rules.py:44\u001B[0m\n\u001B[0;32m     41\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[0;32m     43\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[1;32m---> 44\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_inductor\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtest_operators\u001B[39;00m\n\u001B[0;32m     45\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdistributed\u001B[39;00m\n\u001B[0;32m     46\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_content_store\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_inductor\\test_operators.py:6\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mautograd\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Function\n\u001B[0;32m      5\u001B[0m _test_lib_def \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mlibrary\u001B[38;5;241m.\u001B[39mLibrary(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_inductor_test\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDEF\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m----> 6\u001B[0m _test_lib_def\u001B[38;5;241m.\u001B[39mdefine(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrealize(Tensor self) -> Tensor\u001B[39m\u001B[38;5;124m\"\u001B[39m, tags\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mTag\u001B[38;5;241m.\u001B[39mpt2_compliant_tag)\n\u001B[0;32m      8\u001B[0m _test_lib_impl \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mlibrary\u001B[38;5;241m.\u001B[39mLibrary(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_inductor_test\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIMPL\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m dispatch_key \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCPU\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCUDA\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMeta\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "\u001B[1;31mAttributeError\u001B[0m: type object 'torch._C.Tag' has no attribute 'pt2_compliant_tag'"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a1f8a31c74f9d8a6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
